[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Re-rethinking the causes of deforestation: does the theory evolve?",
    "section": "",
    "text": "Another football field of forest will be lost in:\n\n\n   \n\n6\n\n\n\n\n\nForest loss since you opened this page: 0 acres"
  },
  {
    "objectID": "analysis/6-Assignment2.html",
    "href": "analysis/6-Assignment2.html",
    "title": "Desra Arriyadi",
    "section": "",
    "text": "For Assignment 2, I focused on analyzing data related to population changes in Philadelphia. I created several charts using three tools: Matplotlib, Seaborn, and Altair. Additionally, I built a dashboard using Altair.\nFirst, I visualized overall population data by comparing population change percentages across different races with Matplotlib. Then, I explored causes of mortality in Philadelphia and created related visualizations using Seaborn. For Altair, I made two charts: analyzing the percentage of shooting victims by crime type and analyzing by age. Lastly, I displayed the number of fatalities by age and wound location, displaying in the dashboard.\n\nMatplotlib\n\n\nCode\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\n\nCode\n#Load data\nalldata = pd.read_csv(\"./data/Vital_Population_Cty.csv\")\n\nalldata.head(3)\n\n\n\n\n\n\n\n\n\nOBJECTID\nYEAR\nGEOGRAPHY_NAME\nGEOGRAPHY\nSEX\nRACE_ETHNICITY\nAGE_CATEGORY\nCOUNT_\nSOURCE\n\n\n\n\n0\n1\n2000\nPhiladelphia\nCounty\nAll sexes\nAll races/ethnicities\nAll ages\n1514563\nAnnual County Resident Population Estimates\n\n\n1\n2\n2000\nPhiladelphia\nCounty\nAll sexes\nAsian/PI (NH)\nAll ages\n70128\nAnnual County Resident Population Estimates\n\n\n2\n3\n2000\nPhiladelphia\nCounty\nAll sexes\nBlack (NH)\nAll ages\n650126\nAnnual County Resident Population Estimates\n\n\n\n\n\n\n\n\n\nCode\n#Drop unecessary columns\ncleandata=alldata.drop(['OBJECTID','GEOGRAPHY_NAME','GEOGRAPHY','SOURCE'],axis=1)\n\n\n\n\nCode\n#Only choose all ages, all sexes, and years after 2000\nAllAgesData = cleandata['AGE_CATEGORY'].isin([\"All ages\"]) & cleandata['SEX'].isin([\"All sexes\"])\nphilaAllAges = cleandata.loc[AllAgesData]\n\nexcludebefore2000 = ~philaAllAges['YEAR'].isin([1960,1970,1980,1990])\nphilaData = philaAllAges.loc[excludebefore2000]\n\n\n\n\nCode\n#Create plot\ncolor_map = {\n    \"Asian/PI (NH)\": \"#edc951\", \n    \"Black (NH)\": \"#eb6841\", \n    \"Hispanic\": \"#cc2a36\", \n    \"Multiracial (NH)\": \"#4f372d\", \n    \"White (NH)\": \"#00a0b0\"\n}\n\n#Create the figure and axes\nfig, ax = plt.subplots(figsize=(8, 5))\n\n\n#Iterate over the data, grouped by race ethinicty\nfor iterateRaces, group_df in philaData.groupby(\"RACE_ETHNICITY\"):\n    \n    #Filter out All races\n    if iterateRaces == 'All races/ethnicities':\n        continue\n        \n    group_df = group_df.sort_values(\"YEAR\")\n    \n    #Year vs. population\n    x = group_df[\"YEAR\"]\n    y = group_df[\"COUNT_\"]\n    \n    #Count percentage\n    y_percentage = ((y-y.iloc[0])/y.iloc[0])*100\n    \n    #Plot data line with a specific color\n    ax.plot(\n        x, y_percentage, \n        alpha=0.8, \n        label=iterateRaces, \n        linewidth=2, \n        color=color_map[iterateRaces]\n    )\n    \n    #Text\n    ax.text(\n        x.iloc[-1],\n        y_percentage.iloc[-1]+6,\n        iterateRaces,\n        color=color_map[iterateRaces],\n        fontweight=\"bold\",\n        horizontalalignment=\"center\",\n        verticalalignment=\"center\"\n    )  \n        \n#Format and add a legend\nax.margins(x=0,y=0)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nax.set_title(\"Population change (%) in Philadelphia\", fontsize=18, fontweight=\"bold\")\nax.set_xticks([2000,2005,2010,2015,2020])\nax.set_yticks([-20,0,20,40,60,80,100,120,140])\nplt.xlabel(\"Year\", fontsize=10)\nplt.ylabel(\"Percentage (%)\", fontsize=10)\nplt.tick_params(left = False)\nplt.grid(axis='y', linewidth=0.2)\n\n\n\n\n\n\n\n\n\nThe population graph for Philadelphia illustrates the percentage change by racial/ethnic group each year compare to 2000. Although Philadelphia is known for experiencing overall depopulation, a closer analysis reveals growth in specific ethnic groups. The multiracial non-Hispanic group saw the most significant increase, with a growth of over 120% compared to 2000. Hispanic and Asian populations also experienced notable increases, growing by approximately 90% and 80%, respectively. In contrast, the Black population remained stable between 2000 and 2021, while the White population declined by 20% over the same period. This graph suggests that migration factor may play a role in shaping Philadelphia’s population dynamics. Recently, migration has become a prominent topic in public discussions, particularly in the context of the current election atmosphere.\n\n\nSeaborn Chart\n\n\nCode\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nCode\n#Load data\nmortalitydata = pd.read_csv(\"./data/Vital_Mortality_Cty.csv\")\n\n#Show the first five row\nmortalitydata.head(3)\n\n\n\n\n\n\n\n\n\nOBJECTID\nYEAR\nGEOGRAPHY_NAME\nGEOGRAPHY\nSEX\nRACE_ETHNICITY\nAGE_CATEGORY\nLEADING_CAUSE_DEATH\nMETRIC_NAME\nMETRIC_VALUE\nRANK\nQUALITY_FLAG\nESTIMATE_TYPE\n\n\n\n\n0\n1\n2021\nPhiladelphia\nCounty\nAll sexes\nAll races/ethnicities\nAll ages\nAll alcohol-attributable causes\nalcohol_attributable_deaths\n1235.476033\nNaN\nNaN\nPreliminary\n\n\n1\n2\n2021\nPhiladelphia\nCounty\nAll sexes\nAll races/ethnicities\nAll ages\nAll alcohol-attributable causes\nage_adjusted_alcohol_attributable_mortality_ra...\n73.818445\nNaN\nNaN\nPreliminary\n\n\n2\n3\n2021\nPhiladelphia\nCounty\nAll sexes\nAll races/ethnicities\nAll ages\nAll alcohol-attributable causes\npercent_alcohol_attributable_deaths_out_of_all...\n7.504562\nNaN\nNaN\nPreliminary\n\n\n\n\n\n\n\n\n\nCode\nmortalitydata['LEADING_CAUSE_DEATH'].unique()\n\n\narray(['All alcohol-attributable causes', 'All causes',\n       'Chronic kidney disease', 'Alzheimers disease',\n       'Influenza and pneumonia', 'Intentional self-harm (suicide)',\n       'Chronic liver disease and cirrhosis', 'Parkinson disease',\n       'Drug overdoses', 'Septicemia', 'Homicide',\n       'Chronic lower respiratory diseases', 'HIV/AIDS',\n       'Cerebrovascular diseases',\n       'Pregnancy, childbirth and the puerperium', 'Diabetes', 'Cancer',\n       'Heart disease',\n       'Certain conditions originating in the perinatal period',\n       'COVID-19', 'Unintentional injuries (excluding drug overdoses)',\n       'Congenital malformations, deformations and chromosomal abnormalities',\n       'Breast cancer', 'Colorectal cancer', 'Lung cancer',\n       'Prostate cancer',\n       'Essential (primary) hypertension and hypertensive renal disease',\n       'Anemias', 'Complications of medical and surgical care',\n       'Infections of kidney', 'Diseases of appendix',\n       'In situ neoplasms, benign neoplasms and neoplasms of uncertain or unknown behavior',\n       'Legal intervention', 'Enterocolitis due to Clostridium difficile',\n       'Inflammatory diseases of female pelvic organs',\n       'All smoking-attributable causes', 'Smoking-attributable cancers',\n       'Smoking-attributable cardiovascular and metabolic diseases',\n       'Smoking-attributable pulmonary diseases',\n       'Pneumonitis due to solids and liquids', 'Meningitis',\n       'Tuberculosis', 'Nutritional deficiencies', 'Peptic ulcer',\n       'Acute bronchitis and bronchiolitis', 'Viral hepatitis',\n       'Aortic aneurysm and dissection', 'Atherosclerosis', 'Hernia',\n       'Cholelithiasis and other disorders of gallbladder',\n       'Whooping cough'], dtype=object)\n\n\n\n\nCode\n#Make cause of death group\ngrouped_causes = {\n    \"Cardiovascular Diseases\": [\n        'Heart disease', 'Cerebrovascular diseases', \n        'Essential (primary) hypertension and hypertensive renal disease',\n        'Aortic aneurysm and dissection', 'Atherosclerosis',\n        'Smoking-attributable cardiovascular and metabolic diseases'\n    ],\n    \"Cancers\": [\n        'Cancer', 'Breast cancer', 'Colorectal cancer', 'Lung cancer', \n        'Prostate cancer', 'In situ neoplasms, benign neoplasms and neoplasms of uncertain or unknown behavior',\n        'Smoking-attributable cancers'\n    ],\n    \"Infectious Diseases\": [\n        'COVID-19', 'HIV/AIDS', 'Influenza and pneumonia', 'Septicemia',\n        'Tuberculosis', 'Viral hepatitis', 'Enterocolitis due to Clostridium difficile',\n        'Meningitis', 'Pneumonitis due to solids and liquids', 'Whooping cough'\n    ],\n    \"Chronic and Metabolic Diseases\": [\n        'Chronic kidney disease', 'Alzheimers disease', 'Chronic liver disease and cirrhosis', \n        'Parkinson disease', 'Chronic lower respiratory diseases', 'Diabetes', \n        'Nutritional deficiencies', 'Anemias', 'Peptic ulcer', 'Acute bronchitis and bronchiolitis', \n        'Infections of kidney', 'Cholelithiasis and other disorders of gallbladder', \n        'Hernia', 'Smoking-attributable pulmonary diseases'\n    ],\n    \"External and Other Causes\": [\n        'All causes', 'All alcohol-attributable causes', 'Intentional self-harm (suicide)', \n        'Drug overdoses', 'Homicide', 'Unintentional injuries (excluding drug overdoses)', \n        'Legal intervention', 'Complications of medical and surgical care', \n        'Certain conditions originating in the perinatal period', \n        'Congenital malformations, deformations and chromosomal abnormalities', \n        'Pregnancy, childbirth and the puerperium', 'Inflammatory diseases of female pelvic organs',\n        'Diseases of appendix'\n    ]\n}\n\n\n\n\nCode\ndf4 = pd.DataFrame(mortalitydata)\n\n#Group by 'Region' and 'Product' and count occurrences, count cause death each year\ncauseCounts = df4.groupby(['YEAR','LEADING_CAUSE_DEATH']).size().reset_index(name='Count')\n\n#Display the results\ncauseCounts.head(3)\n\n\n\n\n\n\n\n\n\nYEAR\nLEADING_CAUSE_DEATH\nCount\n\n\n\n\n0\n2012\nAll alcohol-attributable causes\n51\n\n\n1\n2012\nAll causes\n618\n\n\n2\n2012\nAll smoking-attributable causes\n69\n\n\n\n\n\n\n\n\n\nCode\n#Adding group to dataframe\ncause_to_group = {}\nfor group, causes in grouped_causes.items():\n    for cause in causes:\n        cause_to_group[cause] = group\n\n#Map the LEADING_CAUSE_DEATH to its corresponding group\ncauseCounts['causeGroup'] = causeCounts['LEADING_CAUSE_DEATH'].map(cause_to_group)\n\ncauseCounts.head(3)\n\n\n\n\n\n\n\n\n\nYEAR\nLEADING_CAUSE_DEATH\nCount\ncauseGroup\n\n\n\n\n0\n2012\nAll alcohol-attributable causes\n51\nExternal and Other Causes\n\n\n1\n2012\nAll causes\n618\nExternal and Other Causes\n\n\n2\n2012\nAll smoking-attributable causes\n69\nNaN\n\n\n\n\n\n\n\n\n\nCode\n#Define custom colors for each violin plot\ncustom_palette = ['#590404','#A67777','#BFBFBF','#D95F5F','#592F3A' ]\n\nsns.set_theme(style=\"whitegrid\")\n\n#Create the violin plot\nplt.figure(figsize=(10, 5))\nsns.violinplot(data=causeCounts, x='causeGroup', y='Count', palette=custom_palette, inner=\"quart\", \n               linewidth=1)\n\n#Rotate x labels and adjust the font size\nplt.xticks(ha='center', fontsize=7.5, fontweight='bold')\n\n#Set custom y-axis limits\nplt.xlim(-0.5,5)\nplt.ylim(-150,800)\n\n#Remove y grid lines\nplt.grid(False, axis='y')\n\n#Remove y grid lines\nplt.grid(True, axis='x')\n\n#Remove chart borders (spines)\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\n\n#Set labels and title\nplt.xlabel(' ')\nplt.ylabel('Number of cases', fontsize=10)\nplt.title('Mortality by Cause in Philadelphia from 2012 to 2021', fontweight='bold', fontsize=20) \n\n#Create custom text *note:using Matplotlib to get more control\n#Text 1\nx_position1 = 0.1\ny_position1 = 750\n\ncustom_info1 = np.copy(grouped_causes['External and Other Causes'])\ncustom_info1[5] = \" \".join(custom_info1[5].split(\" \")[:2])\ncustom_info1[7] = \" \".join([custom_info1[7].split(\" \")[2], custom_info1[7].split(\" \")[0]])\ncustom_info1[8] = \" \".join(custom_info1[8].split(\" \")[5:])\ncustom_info1[9] = \" \".join(custom_info1[9].split(\" \")[:2])\ncustom_info1[10] = \" \".join(custom_info1[10].split(\" \")[:1])\ncustom_info1[11] = \" \".join(custom_info1[11].split(\" \")[:2])\n\nfor i, line in enumerate(custom_info1):\n    plt.text(x=x_position1, y=y_position1 - (i * 35), s=line, ha='left', fontsize=6, color='#590404') \n\n#Text 2\nx_position2 = 1.1\ny_position2 = 750\n\ncustom_info2 = np.copy(grouped_causes['Chronic and Metabolic Diseases'])\ncustom_info2[11] = custom_info2[11].split(\" \")[0]\ncustom_info2[-1] = \" \".join(custom_info2[-1].split(\" \")[:-1])\n\nfor i, line in enumerate(custom_info2):\n    plt.text(x=x_position2, y=y_position2 - (i * 35), s=line, ha='left', fontsize=6, color='#A67777')  \n    \n#Text 3\nx_position3 = 2.1\ny_position3 = 750\n\ncustom_info3 = np.copy(grouped_causes['Cardiovascular Diseases'])\ncustom_info3[2] = \" \".join(custom_info3[2].split(\" \")[2:6])\ncustom_info3[-1] = \" \".join(custom_info3[-1].split(\" \")[:2])\n\nfor i, line in enumerate(custom_info3):\n    plt.text(x=x_position3, y=y_position3 - (i * 35), s=line, ha='left', fontsize=6, color='#BFBFBF')\n    \n#Tex 4\nx_position4 = 3.1\ny_position4 = 750\n\ncustom_info4 = np.copy(grouped_causes['Cancers'])\ncustom_info4[5] = \" \".join(custom_info4[5].split(\" \")[2:3])\n\nfor i, line in enumerate(custom_info4):\n    plt.text(x=x_position4, y=y_position4 - (i * 35), s=line, ha='left', fontsize=6, color='#D95F5F')\n\n#Text 5\nx_position5 = 4.1\ny_position5 = 750\n\ncustom_info5 = np.copy(grouped_causes['Infectious Diseases'])\n\nfor i, line in enumerate(custom_info5):\n    plt.text(x=x_position5, y=y_position5 - (i * 35), s=line, ha='left', fontsize=6, color='#592F3A')    \n    \n#Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe mortality graph shows the number of deaths across various categories in Philadelphia from 2012 to 2021. Each violin shape represents the distribution of deaths within five categories, with the width of the violin indicating the number of cases at different levels. The dashed line marks the median, while the fine dotted lines represent the quartiles of the data. The “External and Other Causes” category shows a high number of cases, likely because broad categories like “all causes” and “alcohol-attributable causes” are included, resulting in a higher number of cases. The “Cancers” category has the widest violin shape, indicating that the deaths are concentrated around a specific value—approximately 200 cases.\n\n\nAltair\n\n\nCode\nimport altair as alt\n\n\n\n\nCode\n#Load data\nshootingdata = pd.read_csv(\"./data/shootings.csv\")\nshootingdata.head(3)\n\n\n\n\n\n\n\n\n\nthe_geom\nthe_geom_webmercator\nobjectid\nyear\ndc_key\ncode\ndate_\ntime\nrace\nsex\n...\nlocation\nlatino\npoint_x\npoint_y\ndist\ninside\noutside\nfatal\nlat\nlng\n\n\n\n\n0\n0101000020E6100000601EB61D28C852C09D6815059305...\n0101000020110F0000104FDF5323E75FC15005EA2F1496...\n15566955\n2016\n2.016350e+11\n2600.0\n2016-02-14 00:00:00+00\n07:02:00\nB\nF\n...\n400 BLOCK W Fern St\n0.0\n-75.127448\n40.043549\n35.0\n0.0\n1.0\n0.0\n40.043549\n-75.127448\n\n\n1\n0101000020E6100000C5FD89C215CF52C08ED2BE1CEAFB...\n0101000020110F0000CD6B4A0AE8F25FC1FBDF72485E8B...\n15566956\n2022\n2.022190e+11\n100.0\n2022-03-06 00:00:00+00\n08:58:00\nB\nF\n...\n400 BLOCK N 58TH ST\n0.0\n-75.235703\n39.968082\n19.0\n1.0\n0.0\n0.0\n39.968082\n-75.235703\n\n\n2\n0101000020E61000006D8067B55FCD52C0483FF1CEE7FB...\n0101000020110F000039ED49F7FFEF5FC17799D9BA5B8B...\n15566957\n2021\n2.021160e+11\n400.0\n2021-10-20 00:00:00+00\n10:07:00\nB\nM\n...\n4200 BLOCK PARRISH ST APT 203\n0.0\n-75.208967\n39.968012\n16.0\n1.0\n0.0\n0.0\n39.968012\n-75.208967\n\n\n\n\n3 rows × 25 columns\n\n\n\n\n\nCode\n#drop unecessary columns\nshootingclean=shootingdata.drop(\n    ['the_geom',\n     'the_geom_webmercator',\n     'objectid',\n     'dc_key',\n     'point_x',\n     'point_y',\n     'lat',\n     'lng'\n    ],axis=1)\n\n\n\n\nCode\ndf = pd.DataFrame(shootingclean)\n\n#Make groups based on the FBI's Crime Reporting (UCR) system\n#0100 – 0119: Homicide, 0200 – 0299: Rape, 0300 – 0399: Robbery, 0400 – 0499: Aggravated Assault, 3000 – 3900: Hospital Cases\nbins = [99, 199, 299, 399, 499, 2999, 3900] \nlabels = ['Homicide', 'Rape', 'Robbery', 'Aggravated assault', 'Others', 'Hospital cases']\n\ndf['crimeGroup'] = pd.cut(df['code'], bins=bins, labels=labels, right=False)\n\nshootingclean.head(3)\n\n\n\n\n\n\n\n\n\nyear\ncode\ndate_\ntime\nrace\nsex\nage\nwound\nofficer_involved\noffender_injured\noffender_deceased\nlocation\nlatino\ndist\ninside\noutside\nfatal\ncrimeGroup\n\n\n\n\n0\n2016\n2600.0\n2016-02-14 00:00:00+00\n07:02:00\nB\nF\n26.0\nStomach\nN\nN\nN\n400 BLOCK W Fern St\n0.0\n35.0\n0.0\n1.0\n0.0\nOthers\n\n\n1\n2022\n100.0\n2022-03-06 00:00:00+00\n08:58:00\nB\nF\n31.0\nChest\nN\nN\nN\n400 BLOCK N 58TH ST\n0.0\n19.0\n1.0\n0.0\n0.0\nHomicide\n\n\n2\n2021\n400.0\n2021-10-20 00:00:00+00\n10:07:00\nB\nM\n25.0\nMultiple\nN\nN\nN\n4200 BLOCK PARRISH ST APT 203\n0.0\n16.0\n1.0\n0.0\n0.0\nAggravated assault\n\n\n\n\n\n\n\n\n\nCode\ngroupCounts = shootingclean['crimeGroup'].value_counts()\n\n\n\n\nCode\n#Calculate the total count\ntotal_count = groupCounts.sum()\n\n#Calculate percentages\ngroupPercentage = (groupCounts / total_count) * 100\ngroupPercentage.round(1)\n\n\nAggravated assault    66.3\nHomicide              24.6\nRobbery                4.8\nHospital cases         3.2\nOthers                 1.1\nRape                   0.0\nName: crimeGroup, dtype: float64\n\n\n\n\nCode\n#Calculate crime rate in total 10 years, all years: 2015-2014\nrateCrime = groupCounts/10\n\n\n\n\nCode\ncrimeData = {'Count': groupCounts, 'Percentage': groupPercentage.round(2)}\ndf2 = pd.DataFrame(crimeData)\n\ndf2.head(3)\n\n\n\n\n\n\n\n\n\nCount\nPercentage\n\n\n\n\nAggravated assault\n10673\n66.34\n\n\nHomicide\n3953\n24.57\n\n\nRobbery\n768\n4.77\n\n\n\n\n\n\n\n\n\nCode\n#Define custom colors for each violin plot\ncustomPieColor = ['#590404','#A67777','#4B0082','#D95F5F','#2F4F4F','#BFBFBF']\n\n#Create a point selection for the pie chart\nbrush = alt.selection_point(fields=['crimeGroup'], encodings=['color'])\n\n#Create the data for the pie chart using 'Percentage'\nsource = pd.DataFrame({\n    'crimeGroup': df2.index,\n    'values': df2['Percentage']\n})\n\n#Create the base chart with a legend for the crime group categories\nbase = alt.Chart(source).encode(\n    alt.Theta(\"values:Q\").stack(True),\n    alt.Radius(\"values:Q\").scale(type=\"sqrt\", zero=True, rangeMin=20),\n    color=alt.condition(\n        brush,\n        alt.Color(\"crimeGroup:N\",\n                  scale=alt.Scale(range=customPieColor),\n                  legend=alt.Legend(title=\"Crime Type\")),\n        alt.value('lightgray')\n    )\n)\n\n#Create the pie chart with arcs\nc1 = base.mark_arc(innerRadius=20, stroke=\"#fff\").add_params(brush).properties(width=350, height=350)\n\n#Create text marks for percentages with a percentage symbol, initially hidden and shown only on hover\nc2 = base.mark_text(radiusOffset=45, fontWeight=\"bold\").encode(\n    text=alt.Text(\"values:Q\", format=\".2f\"),\n    opacity=alt.condition(brush, alt.value(1), alt.value(0))\n).transform_calculate(\n    label=\"datum.values + '%'\"\n).encode(\n    text='label:N'\n)\n\n#Combine the arc and text\nchart = c1 + c2\n\n#Add tooltip for interactivity on hover\ntooltip = alt.Tooltip('crimeGroup:N', title='Crime Group')\n\n#Combine the arc and text with tooltips\nchart = chart.encode(tooltip=tooltip)\n\n#Add a title to the chart\nchart = chart.properties(\n    title={\n        \"text\": \"Shooting Victims Percentage by Crime Type\",\n        \"dx\": 50,\n        \"dy\": -15,\n        \"fontSize\": 20,\n    }\n)\n\n#Display the chart\nchart\n\n\n\n\n\n\n\n\nThe chart above illustrates the percentage of shooting victims across various crime categories in Philadelphia between 2015 and 2024. Aggravated assault accounts for approximately 66.34%, making it the most prevalent crime associated with shootings. Homicide is the second most significant, representing 24.57%. Other crime types contribute much smaller percentages compared to these two major categories. This chart indicates that gun violence in Philadelphia is a serious concern. The next graph will provide further details by examining shooting victims based on age.\n\n\nCode\ndf3 = pd.DataFrame(shootingclean)\n\n#Count\nraceCounts = df3.groupby(['year','age']).size().reset_index(name='Count')\n\n#Display the results\nraceCounts.head(3)\n\n\n\n\n\n\n\n\n\nyear\nage\nCount\n\n\n\n\n0\n2015\n1.0\n1\n\n\n1\n2015\n3.0\n1\n\n\n2\n2015\n6.0\n1\n\n\n\n\n\n\n\n\n\nCode\n#Brush for selection\nbrush2 = alt.selection_interval()\n\n#Scatter Plot\npoints = alt.Chart(raceCounts).mark_circle().encode(\n    x=alt.X('year:Q', scale=alt.Scale(domain=[2014, 2025]), \n            axis=alt.Axis(format='.0f', grid=False)),\n    y=alt.Y('age:Q', \n            axis=alt.Axis(domain=False, ticks=False, grid=False)),\n    size=alt.Size('Count:Q',\n                  legend=alt.Legend(\n                      title=\"\", \n                      symbolType='circle',\n                      orient='top',\n                      offset=-10\n                  )),\n    color=alt.condition(brush2, alt.value('red'), alt.value('grey')),\n    opacity=alt.condition(brush2, alt.value(0.3), alt.value(0.2))\n).add_params(brush2)\n\n#Add a solid black line at age 21\nline_at_age_21 = alt.Chart(pd.DataFrame({'age': [21]})).mark_rule(color='black', opacity = 0.5,strokeWidth=3).encode(\n    y='age:Q'\n)\n\n#Add text to the right of the line\ntext_below_line = alt.Chart(pd.DataFrame({'age': [21], 'text': ['Age 21']})).mark_text(\n    align='left',\n    baseline='middle',\n    dy=10,\n    fontWeight='bold'\n).encode(\n    y='age:Q',\n    x=alt.value(280),\n    text='text:N'\n)\n\n#Base chart for data tables\nranked_text = alt.Chart(raceCounts).mark_text(align='right', color='black').encode(\n    y=alt.Y('row_number:O').axis(None)\n).transform_filter(\n    brush2\n).transform_window(\n    row_number='row_number()'\n).transform_filter(\n    alt.datum.row_number &lt; 17\n)\n\n#Data Tables\nyeartable = ranked_text.encode(text='age:N').properties(\n    title=alt.Title(text='Age', align='right', color ='black')\n)\nagetable = ranked_text.encode(text='year:N').properties(\n    title=alt.Title(text='Year', align='right', color ='black')\n)\ntext = alt.hconcat(yeartable, agetable)\n\n#Build chart\nalt.hconcat(\n    points + line_at_age_21 + text_below_line,\n    text\n).resolve_legend(\n    color=\"independent\"\n).configure_view(\n    stroke=None \n).properties(\n    title={\n        \"text\": [\"Shooting Victims by Age\"],\n        \"anchor\": \"middle\",\n        \"align\": \"left\",\n        \"dx\": -170,\n        \"dy\": -7,\n        \"fontSize\": 30,\n        \"fontWeight\": \"bold\",\n    }\n)\n\n\n\n\n\n\n\n\nThe chart above displays the number of shooting victims by age from 2015 to 2024. The size of each red dot represents the number of victims, while the black line marks age 21, the minimum legal age to obtain a gun. The results show that the highest concentration of victims is clustered around teenagers and individuals in their early twenties. The pattern of victims remains consistent each year, resembling a “pine tree” shape. What is particularly striking is the consistency of this pattern over the years, with a larger “lower canopy” indicating that more young people are commonly affected by shootings. This raises curiosity about how the pattern might look on a larger scale, such as at the state or national level, and how the data on suspected gun shooters might compare.\n\n\nAltair Dashboard\n\n\nCode\nshootingclean['wound'].unique()\n\n\narray(['Stomach', 'Chest', 'Multiple', 'Head', 'Leg', 'Abdomen', 'Back',\n       'Groin', 'Shoulder', 'Hand', 'Multiple/Head', 'Hip', 'Foot', 'Arm',\n       'back', 'Buttocks', 'Neck', 'Wrist', 'Ankle', 'Unknown', 'ARM',\n       'leg', 'LEG', 'MULTI', 'HEAD', 'buttocks', 'head', 'arm', nan,\n       'Pelvis', 'CHEST', 'THIGH', 'SHOULDER', 'hand', 'chest', 'NECK',\n       'ANKLE', 'foot', 'ankle', 'MULTIPLE/HEAD', 'ABDOMEN', 'stomach',\n       'MULTIPLE', 'abdomen', 'LEGS', 'shoulder', 'FOOT', 'pelvis',\n       'BACK', 'hip', 'neck', 'STOMACH', 'KNNES', 'groin', 'HAND',\n       'Multiple/HEAD', 'BUTTOCKS', 'multi', 'TORSO'], dtype=object)\n\n\n\n\nCode\n#Create categories\nheadParts = ['Head', 'head', 'HEAD', 'Neck', 'neck', 'NECK', 'Multiple/Head', 'MULTIPLE/HEAD', 'Multiple/HEAD']\nupperBodyParts = ['Stomach', 'stomach', 'STOMACH', 'Abdomen', 'abdomen', 'ABDOMEN', 'Chest', 'chest', 'CHEST',\n                   'Back', 'back', 'BACK', 'Groin', 'groin', 'Shoulder', 'shoulder', 'SHOULDER', 'Buttocks', \n                   'buttocks', 'BUTTOCKS', 'Arm', 'arm', 'ARM', 'Wrist', 'Hand', 'hand', 'HAND', 'TORSO', \n                   'Pelvis', 'pelvis', 'Hip', 'hip', 'MULTI', 'Multiple', 'Unknown', 'nan', 'MULTIPLE', 'multi']\nlowerBodyParts = ['Leg', 'leg', 'LEG', 'Legs', 'LEGS', 'Foot', 'foot', 'FOOT', 'Ankle', 'ankle', 'ANKLE', \n             'THIGH', 'KNNES']\n\n\n\n\nCode\n#make all data as Upper Body by default, assuming multple, nan, unknown, etc are upper body\nshootingclean['bodyParts'] = \"Upper Body\"\n\n\n\n\nCode\n#Add two other categories categories: Head and Lowe Body\nhead_rows = shootingclean['wound'].isin(headParts)\nshootingclean.loc[head_rows, 'bodyParts'] = 'Head'\n\nupperBody_rows = shootingclean['wound'].isin(upperBodyParts)\nshootingclean.loc[upperBody_rows, 'bodyParts'] = 'Upper Body'\n\nlowerBody_rows = shootingclean['wound'].isin(lowerBodyParts)\nshootingclean.loc[lowerBody_rows, 'bodyParts'] = 'Lower Body'\n\nshootingclean.head(3)\n\n\n\n\n\n\n\n\n\nyear\ncode\ndate_\ntime\nrace\nsex\nage\nwound\nofficer_involved\noffender_injured\noffender_deceased\nlocation\nlatino\ndist\ninside\noutside\nfatal\ncrimeGroup\nbodyParts\n\n\n\n\n0\n2016\n2600.0\n2016-02-14 00:00:00+00\n07:02:00\nB\nF\n26.0\nStomach\nN\nN\nN\n400 BLOCK W Fern St\n0.0\n35.0\n0.0\n1.0\n0.0\nOthers\nUpper Body\n\n\n1\n2022\n100.0\n2022-03-06 00:00:00+00\n08:58:00\nB\nF\n31.0\nChest\nN\nN\nN\n400 BLOCK N 58TH ST\n0.0\n19.0\n1.0\n0.0\n0.0\nHomicide\nUpper Body\n\n\n2\n2021\n400.0\n2021-10-20 00:00:00+00\n10:07:00\nB\nM\n25.0\nMultiple\nN\nN\nN\n4200 BLOCK PARRISH ST APT 203\n0.0\n16.0\n1.0\n0.0\n0.0\nAggravated assault\nUpper Body\n\n\n\n\n\n\n\n\n\nCode\ndf5 = pd.DataFrame(shootingclean)\n\n#Make groups based on UCR systemc\nbins = [0, 11, 21, 31, 41, 51, 61, 71, 120] \nlabels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60','61-70','&gt;70']\n\ndf5['ageGroup'] = pd.cut(df5['age'], bins=bins, labels=labels, right=False)\n\nshootingclean.head(3)\n\n\n\n\n\n\n\n\n\nyear\ncode\ndate_\ntime\nrace\nsex\nage\nwound\nofficer_involved\noffender_injured\noffender_deceased\nlocation\nlatino\ndist\ninside\noutside\nfatal\ncrimeGroup\nbodyParts\nageGroup\n\n\n\n\n0\n2016\n2600.0\n2016-02-14 00:00:00+00\n07:02:00\nB\nF\n26.0\nStomach\nN\nN\nN\n400 BLOCK W Fern St\n0.0\n35.0\n0.0\n1.0\n0.0\nOthers\nUpper Body\n21-30\n\n\n1\n2022\n100.0\n2022-03-06 00:00:00+00\n08:58:00\nB\nF\n31.0\nChest\nN\nN\nN\n400 BLOCK N 58TH ST\n0.0\n19.0\n1.0\n0.0\n0.0\nHomicide\nUpper Body\n31-40\n\n\n2\n2021\n400.0\n2021-10-20 00:00:00+00\n10:07:00\nB\nM\n25.0\nMultiple\nN\nN\nN\n4200 BLOCK PARRISH ST APT 203\n0.0\n16.0\n1.0\n0.0\n0.0\nAggravated assault\nUpper Body\n21-30\n\n\n\n\n\n\n\n\n\nCode\n#Group\nwoundCounts = shootingclean.groupby(['bodyParts','ageGroup']).size().reset_index(name='Count')\n\n#Display the results\nwoundCounts.head(3)\n\n\n\n\n\n\n\n\n\nbodyParts\nageGroup\nCount\n\n\n\n\n0\nHead\n0-10\n20\n\n\n1\nHead\n11-20\n414\n\n\n2\nHead\n21-30\n825\n\n\n\n\n\n\n\n\n\nCode\n#Create an interactive selection for body parts (acting as clickable elements)\ndataSelection = alt.selection_point(\n    fields=['bodyParts'],\n    on=\"click\",\n    empty=True\n)\n\nchart3 = (\n    alt.Chart(woundCounts)\n    .mark_bar()\n    .encode(\n        x=alt.X(\"Count:Q\",title=\"Number of victims\"),\n        y=alt.Y(\"ageGroup:N\",title=\"Age\"),\n        color=alt.condition(\n            dataSelection,\n            alt.Color(\n                \"bodyParts:N\",\n                scale=alt.Scale(\n                    domain=['Head', 'Upper Body', 'Lower Body'],\n                    range=['#590404','#D95F5F','#A67777']\n                ),\n                legend=None,\n            ),\n            alt.value(\"lightgray\")\n        )\n    )\n    .properties(width=800)\n    .add_params(dataSelection)\n)\n\nchart1 = (\n    alt.Chart(woundCounts)\n    .mark_bar()\n    .encode(\n        x=alt.X(\"ageGroup:N\", axis=alt.Axis(labelAngle=0),title=\"Age\"),\n        y=alt.Y(\"bodyParts:N\", sort=['Head', 'Upper Body', 'Lower Body'],title=\"\"),\n        color=alt.condition(\n            dataSelection,\n            alt.Color(\n                \"Count:Q\",\n                scale=alt.Scale(\n                    range=['#E1B799', '#EC8461', '#FF2828', '#D71B1B', '#A90100', '#560505']\n                ),\n                legend=alt.Legend(\n                    title=\"Victims\",\n                    titleFontSize=12,\n                    labelFontSize=10,\n                    symbolSize=10,\n                    offset=-140\n                )\n            ),\n            alt.value(\"lightgray\")\n        ),\n    )\n    .properties(width=300, height=180)\n    .add_params(dataSelection)\n)\n\n#Create a DataFrame with only one data point\ndataPerson = pd.DataFrame([{'bodyParts': 'Whole Body'}])\n\n#Sample data for body parts: Head, Body, and Legs\ndata_parts = pd.DataFrame({\n    'bodyParts': ['Head', 'Upper Body', 'Lower Body'],\n    'x': [1, 1, 1],\n    'y': [3, 2, 1],\n    'Count': [1, 1, 1]\n})\n\n#SVG path for a person shape\nperson = (\n    \"M1.3 -1.7h-0.8c0.3 -0.2 0.6 -0.5 0.6 -0.9c0 -0.6 \"\n    \"-0.4 -1 -1 -1c-0.6 0 -1 0.4 -1 1c0 0.4 0.2 0.7 0.6 \"\n    \"0.9h-0.8c-0.4 0 -0.7 0.3 -0.7 0.6v1.9c0 0.3 0.3 0.6 \"\n    \"0.6 0.6h0.2c0 0 0 0.1 0 0.1v1.9c0 0.3 0.2 0.6 0.3 \"\n    \"0.6h1.3c0.2 0 0.3 -0.3 0.3 -0.6v-1.8c0 0 0 -0.1 0 \"\n    \"-0.1h0.2c0.3 0 0.6 -0.3 0.6 -0.6v-2c0.2 -0.3 -0.1 \"\n    \"-0.6 -0.4 -0.6z\"\n)\n\n#Chart for the person shape\nperson_chart = alt.Chart(dataPerson).mark_point(\n    filled=True,\n    size=1500,\n    color='grey'\n).encode(\n    alt.ShapeValue(person)\n)\n\n#Create the bar chart for head, body, and legs\nparts_chart = alt.Chart(data_parts).mark_bar(\n    size=100,\n    fillOpacity=0,\n    strokeWidth=2\n).encode(\n    x=alt.X('x:O', axis=None),\n    y=alt.Y('y:O', axis=None, sort='descending', bandPosition=0),\n    stroke=alt.condition(\n        dataSelection,\n        alt.Color(\n            \"bodyParts:N\",\n            scale=alt.Scale(\n                domain=['Head', 'Upper Body', 'Lower Body'],\n                range=['#590404','#D95F5F','#A67777']\n            ),\n            legend=None\n        ),\n        alt.value('lightgray'), \n    ),\n    strokeOpacity=alt.condition(\n        dataSelection, \n        alt.value(1),\n        alt.value(0)\n    ),\n    tooltip=['bodyParts']\n).properties(\n    width=80,\n    height=180\n).add_params(\n    dataSelection\n)\n\n#Combine all the charts: person, head, body, and legs\nchart2 = person_chart + parts_chart\n\n#Combine chart1 and chart2 horizontally\nhorizontal_concat = alt.hconcat(chart1, chart2, spacing=80).properties(\n    title={\n        \"text\": [\"Click the corresponding body part to specify data.\"],\n        \"anchor\": \"middle\",\n        \"align\": \"left\",\n        \"dx\": 285,\n        \"dy\": 170,\n        \"fontSize\": 12,\n        \"fontWeight\": \"normal\",\n    }\n)\n\n#Vertically combine the horizontal_concat and chart3\ndashboard = alt.vconcat(horizontal_concat, chart3).properties(\n    title={\n        \"text\": [\"Philadelphia's\",\"Shooting Victims\",\"2015-2024\"],\n        \"anchor\": \"middle\",\n        \"align\": \"left\",\n        \"dx\": 110,\n        \"dy\": 130,\n        \"fontSize\": 41,\n        \"fontWeight\": \"bold\",\n    }\n)\n\n#Display the concatenated charts\ndashboard\n\n\n\n\n\n\n\n\nThe dashboard provides an overview of Philadelphia’s shooting victims, categorized by age and wound location. Wound locations are grouped into three categories: head, upper body, and lower body. In the heatmap, darker shades indicate a higher number of victims, with the upper body for victims aged 21-30 being the most impacted group.",
    "crumbs": [
      "Analysis",
      "Desra Arriyadi"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium.",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\n\nCode\nimport osmnx as ox\n\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\n\nCode\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\n\n\nCode\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\n\n\nCode\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\n\nCode\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\n\n\nCode\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\n\nNext, use osmnx to download the street graph around Center City.\n\n\nCode\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n\nCode\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\n\nFind the shortest path, based on the distance of the edges:\n\n\nCode\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\n\nCode\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n\nCode\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n\n\nCode\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\n\nCode\nimport folium\nimport xyzservices\n\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n\nCode\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nCode\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\nCode\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\nCode\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n\nCode\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n\nCode\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n\nCode\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n\nCode\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Thoughts",
    "section": "",
    "text": "©Photo by Desra\n\n In 1999, researchers from the World Bank published an article titled “Rethinking the causes of deforestation: lesson from economic models”. This article synthesized the causes of deforestation based on hundreds of economic models available at that time. They highlighted a logical approach to analyzing deforestation at three different levels: sources (small farmers, loggers, etc.), immediate causes (prices, wages, technology, accessibility, etc.), and underlying causes (population, economic growth). A clear distinction among the three levels was based on the scale, from micro to macro influences. The findings showed only weak support for the thesis that population growth and poverty are driving forces of deforestation. Economic growth often drives deforestation by influencing higher agricultural and timber prices, while lower timber prices tend to reduce logging and agricultural encroachment. The tenure thesis suggests that secure land tenure can influence deforestation by incentivizing forest clearing. The intensification thesis revealed a mixed effect, as agricultural advancements may either exacerbate or alleviate deforestation.\nInspired by this article, I plan to “re-rethink” the variables that influence deforestation. To conduct this research, I plan to use the World Bank’s data API, a Python library that provides access to a wide range of global development data. This will allow me to analyze updated datasets and examine the impact of various factors (for example, population growth, economic development, agriculture, and other variables) on deforestation. My objectives are to identify key drivers of deforestation in the modern era and evaluate whether the findings from the 1999 World Bank’s article remain relevant today.\nThis study will use the Random Forest method to analyze the impact of various key drivers on deforestation. The process will be carried out in the following steps:\n\n\n\n\n\n\n A total of 65 variables were selected and classified into 10 topic groups, including:\nAgriculture and land use\nAgricultural land (sq. km)\nForest area (% of land area)\nAgricultural land (% of land area)\nArable land (hectares)\nPermanent cropland (% of land area)\nForest area (sq. km)\nUrban land area (sq. km)\nCereal production (metric tons)\nCrop production index (2014-2016 = 100)\nFertilizer consumption (% of fertilizer production)\n\nEconomy\nAgriculture, forestry, and fishing, value added (% of GDP)\nAdjusted savings: net forest depletion (current USD)\nForest rents (% of GDP)\nGDP growth (annual %)\nAgricultural raw materials imports (% of merchandise imports)\nImport value index (2015 = 100)\nAgricultural raw materials exports (% of merchandise exports)\nTrade in services (% of GDP)\nPrimary income payments (BoP, current USD)\nTravel services (% of service imports, BoP)\nExports of goods and services (BoP, current USD)\nMarket capitalization of listed domestic companies (current USD)\nExternal debt stocks, total (DOD, current USD)\nUse of IMF credit (DOD, current USD)\nDebt service on external debt, total (TDS, current USD)\nMultilateral debt service (TDS, current USD)\nBorrowers from commercial banks (per 1,000 adults)\nNet domestic credit (current LCU)\nNet foreign assets (current LCU)\nConsumer price index (2010 = 100)\nTaxes on exports (current LCU)\nTaxes on goods and services (current LCU)\nTaxes on international trade (current LCU)\nSubsidies and other transfers (current LCU)\nLead time to export, median case (days)\nGDP (current USD)\n\nEducation\nLiteracy rate, adult total (% of people ages 15 and above)\nSchool enrollment, primary (% gross)\nSchool enrollment, secondary (% gross)\n\nEmployment\nChild employment in agriculture (% of economically active children ages 7-14)\nEmployment in agriculture (% of total employment) (modeled ILO estimate)\nSelf-employed, total (% of total employment) (modeled ILO estimate)\nLabor force, total\n\nEnergy\nFossil fuel energy consumption (% of total)\nEnergy use (kg of oil equivalent per capita)\n\nHealth\nPrevalence of underweight, weight for age (% of children under 5)\nPrevalence of undernourishment (% of population)\n\nInequality\nPoverty headcount ratio at D2.15 a day (2017 PPP) (% of population)\nGini index\n\nInstitutional quality\nGovernment Effectiveness: Estimate\nRegulatory Quality: Estimate\nControl of Corruption: Estimate\n\nPopulation\nPopulation density (people per sq. km of land area)\nPopulation growth (annual %)\nPopulation, total\nRural population\nRural population growth (annual %)\nRural population (% of total population)\nUrban population growth (annual %)\nUrban population (% of total population)\nRural land area (sq. km)\nHuman capital index (HCI) (scale 0-1)\nRail lines (total route-km)\n\nStability\nPolitical Stability and Absence of Violence/Terrorism: Estimate\nInternally displaced persons, total displaced by conflict and violence (number of people)\n\n\nReference\nAngelsen, A., & Kaimowitz, D. (1999). Rethinking the Causes of Deforestation: Lessons from Economic Models. The World Bank Research Observer, 14(1), 73–98. https://doi.org/10.1093/wbro/14.1.73."
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Analysis",
      "Python code blocks"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot.",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\n\nCode\nimport altair as alt\n\n\nAnd generate our final data viz:\n\n\nCode\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n\nCode\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/5-assignment-1.html",
    "href": "analysis/5-assignment-1.html",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "",
    "text": "In this assignment, we will practice our pandas skills and explore the “Donut Effect” within Philadelphia. The “Donut Effect” describes the following phenomenon: with more flexible working options and pandemic-driven density fears, people left urban dense cores and opted for more space in city suburbs, driving home and rental prices up in the suburbs relative to city centers.\nWe will be working with Zillow data for the Zillow Home Value Index (ZHVI) for Philadelphia ZIP codes. The goal will be to calculate home price appreciation in Philadelphia, comparing those ZIP codes in Center City (the central business district) to those not in Center City.",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/5-assignment-1.html#load-the-data",
    "href": "analysis/5-assignment-1.html#load-the-data",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "1. Load the data",
    "text": "1. Load the data\nI’ve already downloaded the relevant data file and put in the data/ folder. Let’s load it using pandas.\nNote: Be sure to use a relative file path to make it easier to load your data when grading. See this guide for more info.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\n#import and check data\nalldata = pd.read_csv(\"data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\nalldata.head(3)\n\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\nState\nCity\nMetro\nCountyName\n2000-01-31\n...\n2021-10-31\n2021-11-30\n2021-12-31\n2022-01-31\n2022-02-28\n2022-03-31\n2022-04-30\n2022-05-31\n2022-06-30\n2022-07-31\n\n\n\n\n0\n61639\n0\n10025\nZip\nNY\nNY\nNew York\nNew York-Newark-Jersey City\nNew York County\n329164.0\n...\n1153364.0\n1152736.0\n1153314.0\n1159292.0\n1171216.0\n1190200.0\n1207107.0\n1221417.0\n1227148.0\n1234232.0\n\n\n1\n84654\n1\n60657\nZip\nIL\nIL\nChicago\nChicago-Naperville-Elgin\nCook County\n311718.0\n...\n523727.0\n526511.0\n528499.0\n529879.0\n530092.0\n532758.0\n534840.0\n539859.0\n543658.0\n546709.0\n\n\n2\n61637\n2\n10023\nZip\nNY\nNY\nNew York\nNew York-Newark-Jersey City\nNew York County\n510209.0\n...\n1517150.0\n1521442.0\n1521759.0\n1532449.0\n1542269.0\n1559390.0\n1572653.0\n1591368.0\n1600569.0\n1607770.0\n\n\n\n\n3 rows × 280 columns",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/5-assignment-1.html#trim-the-data-to-just-philadelphia",
    "href": "analysis/5-assignment-1.html#trim-the-data-to-just-philadelphia",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "2. Trim the data to just Philadelphia",
    "text": "2. Trim the data to just Philadelphia\nSelect the subset of the dataframe for Philadelphia, PA.\n\n\nCode\n#show column list\nalldata.columns\n\n\nIndex(['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName',\n       'State', 'City', 'Metro', 'CountyName', '2000-01-31',\n       ...\n       '2021-10-31', '2021-11-30', '2021-12-31', '2022-01-31', '2022-02-28',\n       '2022-03-31', '2022-04-30', '2022-05-31', '2022-06-30', '2022-07-31'],\n      dtype='object', length=280)\n\n\n\n\nCode\n#trim data to just Pennsylvania, excluding any Philadelphia from other states\npennsyl = alldata[\"State\"].isin([\"PA\"])\ntrim_pennsyl = alldata.loc[pennsyl]\n\n#check unique data selected\ntrim_phila['State'].unique()\n\n\narray(['PA'], dtype=object)\n\n\n\n\nCode\n#trim data to just Philadelphia in Pennsylvania\nphila = trim_pennsyl[\"City\"].isin([\"Philadelphia\"])\ntrim_phila = trim_pennsyl.loc[phila]\n\n#check unique data selected\ntrim_phila['City'].unique()\n\n\narray(['Philadelphia'], dtype=object)",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/5-assignment-1.html#melt-the-data-into-tidy-format",
    "href": "analysis/5-assignment-1.html#melt-the-data-into-tidy-format",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "3. Melt the data into tidy format",
    "text": "3. Melt the data into tidy format\nLet’s transform the data from wide to tidy using the pd.melt() function. Create a new column in your data called “ZHVI” that holds the ZHVI values.\n\n\nCode\n#function that test if a string column name starts with '20'\ndef looks_like_a_date(column_name):\n    return column_name.startswith(\"20\")\n\n\n\n\nCode\n#test the function\nlooks_like_a_date(\"2000-01-31\")\n\n\nTrue\n\n\n\n\nCode\n#get values by explicitly pass to list() function\nphilalist = list(\n    filter(looks_like_a_date, trim_phila.columns)\n)\n\n#remove hashtag symbol (#) below to check\n#philalist\n\n\n\n\nCode\n#use melt function for converting from wide formats to tidy formats\npd.melt(\n    trim_phila,\n    id_vars=[\"RegionName\", \"Metro\", \"CountyName\"],\n    value_vars=list(\n        filter(looks_like_a_date, trim_phila.columns)\n    ),\n    var_name=\"Date\",\n    value_name=\"ZHVI\",\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 2\n      1 #use melt function for converting from wide formats to tidy formats\n----&gt; 2 pd.melt(\n      3     trim_phila,\n      4     id_vars=[\"RegionName\", \"Metro\", \"CountyName\"],\n      5     value_vars=list(\n      6         filter(looks_like_a_date, trim_phila.columns)\n      7     ),\n      8     var_name=\"Date\",\n      9     value_name=\"ZHVI\",\n     10 )\n\nNameError: name 'pd' is not defined\n\n\n\n\n\nCode\n#apply melt function\nphila_tidy = trim_phila.melt(\n    id_vars=[\"RegionName\",\"Metro\", \"CountyName\"],\n    value_vars=list(filter(looks_like_a_date, trim_phila.columns)),\n    var_name=\"Date\",\n    value_name=\"ZHVI\",\n)\n\n#check\nphila_tidy.head(5)\n\n\n\n\n\n\n\n\n\nRegionName\nMetro\nCountyName\nDate\nZHVI\n\n\n\n\n0\n19143\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n2000-01-31\n60701.0\n\n\n1\n19111\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n2000-01-31\n85062.0\n\n\n2\n19124\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n2000-01-31\n47155.0\n\n\n3\n19120\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n2000-01-31\n59285.0\n\n\n4\n19104\nPhiladelphia-Camden-Wilmington\nPhiladelphia County\n2000-01-31\n74255.0",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/5-assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "href": "analysis/5-assignment-1.html#split-the-data-for-zip-codes-inoutside-center-city",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "4. Split the data for ZIP codes in/outside Center City",
    "text": "4. Split the data for ZIP codes in/outside Center City\nTo compare home appreciation in Center City vs. outside Center City, we’ll need to split the data into two dataframes, one that holds the Center City ZIP codes and one that holds the data for the rest of the ZIP codes in Philadelphia.\nTo help with this process, I’ve included a list of ZIP codes that make up the “greater Center City” region of Philadelphia. Use this list to split the melted data into two dataframes.\n\n\nCode\n#first, check the total number of unique ZIP codes\nphila_tidy['RegionName'].nunique()\n\n\n46\n\n\n\n\nCode\n#List of the greater Center City\ngreater_center_city_zip_codes = [\n    19123,\n    19102,\n    19103,\n    19106,\n    19107,\n    19109,\n    19130,\n    19146,\n    19147,\n]\n\n#there are total 9 ZIP codes. Remember, the count starts from 0, so it is equal to 8 in Python\n\n\n\n\nCode\n#split the data for ZIP codes in Center City\nphila_center_sel = phila_tidy['RegionName'].isin(greater_center_city_zip_codes)\nphila_center = phila_tidy.loc[phila_center_sel]\n\n#verify the total number of unique ZIP codes in Center City\nphila_center['RegionName'].nunique()\n\n\n8\n\n\n\n\nCode\n#split the data for ZIP codes outside Center City by adding tildes symbol (~) to invert the result\nphila_outside_sel = ~phila_tidy['RegionName'].isin(greater_center_city_zip_codes)\nphila_outside = phila_tidy.loc[phila_outside_sel]\n\n#verify the total number of unique ZIP codes outside Center City\nphila_outside['RegionName'].nunique()\n\n\n38",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/5-assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "href": "analysis/5-assignment-1.html#compare-home-value-appreciation-in-philadelpia",
    "title": "Assignment #1: The Donut Effect for Philadelphia ZIP Codes",
    "section": "5. Compare home value appreciation in Philadelpia",
    "text": "5. Compare home value appreciation in Philadelpia\nIn this step, we’ll calculate the average percent increase in ZHVI from March 2020 to March 2022 for ZIP codes in/out of Center City. We’ll do this by:\n\nWriting a function (see the template below) that will calculate the percent increase in ZHVI from March 31, 2020 to March 31, 2022\nGroup your data and apply this function to calculate the ZHVI percent change for each ZIP code in Philadelphia. Do this for both of your dataframes from the previous step.\nCalculate the average value across ZIP codes for both sets of ZIP codes and then compare\n\nYou should see much larger growth for ZIP codes outside of Center City…the Donut Effect!\n\n\nCode\n#use groupby operation for Center City\ncenter_grouped = phila_center.groupby(\"RegionName\")\ncenter_grouped\n\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001C8CD407C10&gt;\n\n\n\n\nCode\n#use groupby operation for outside of Center City\noutside_grouped = phila_outside.groupby(\"RegionName\")\noutside_grouped\n\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001C8CD4079D0&gt;\n\n\n\n\nCode\n#set up the function to calculate percent increase\ndef calculate_percent_increase(group_df):\n    \"\"\"\n    Calculate the percent increase from 2020-03-31 to 2022-03-31.\n    \n    Note that `group_df` is the DataFrame for each group.\n    \"\"\"\n    \n    #create selections for the march 2020 and march 2022 data\n    march2020_sel = group_df[\"Date\"] == \"2020-03-31\"\n    march2022_sel = group_df[\"Date\"] == \"2022-03-31\"\n    \n    #get the data for each month (only 1 row, so squeeze it!)\n    march_2020 = group_df.loc[march2020_sel].squeeze()\n    march_2022 = group_df.loc[march2022_sel].squeeze()\n\n    #columns to calculate percent change for\n    columns = [\"ZHVI\"]\n    \n    #return the percent change for both columns\n    return 100 * (march_2022[columns] / march_2020[columns] - 1)\n\n\n\n\nCode\n#apply function for Center City\ncenter_result = center_grouped.apply(calculate_percent_increase)\ncenter_result.round(2).sort_values(by=\"ZHVI\", ascending=True)\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19102\n-1.72\n\n\n19103\n-1.70\n\n\n19106\n2.52\n\n\n19107\n2.88\n\n\n19123\n5.21\n\n\n19147\n6.14\n\n\n19146\n6.48\n\n\n19130\n6.67\n\n\n\n\n\n\n\n\n\nCode\n#apply function for outside of Center City\noutside_result = outside_grouped.apply(calculate_percent_increase)\noutside_result.round(2).sort_values(by=\"ZHVI\", ascending=True)\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19148\n6.96\n\n\n19145\n7.63\n\n\n19122\n10.28\n\n\n19125\n11.01\n\n\n19104\n14.54\n\n\n19129\n15.60\n\n\n19119\n17.48\n\n\n19118\n17.59\n\n\n19154\n17.93\n\n\n19150\n18.74\n\n\n19151\n19.65\n\n\n19127\n20.02\n\n\n19126\n20.82\n\n\n19114\n21.07\n\n\n19144\n21.09\n\n\n19128\n21.89\n\n\n19152\n21.99\n\n\n19115\n22.46\n\n\n19116\n23.08\n\n\n19137\n23.25\n\n\n19131\n23.36\n\n\n19134\n23.94\n\n\n19143\n23.95\n\n\n19138\n24.66\n\n\n19149\n24.92\n\n\n19121\n26.23\n\n\n19141\n26.44\n\n\n19136\n26.49\n\n\n19120\n26.93\n\n\n19135\n28.12\n\n\n19111\n28.69\n\n\n19124\n28.74\n\n\n19133\n36.14\n\n\n19139\n37.01\n\n\n19153\n38.24\n\n\n19142\n44.56\n\n\n19140\n57.15\n\n\n19132\n72.22\n\n\n\n\n\n\n\n\n\nCode\n#Calculate the average of ZVHI for Center City\n#first trim to dates from March 2020 onwards, then group by county name\ncenter_group_avg = phila_center.query(\"Date &gt;= '2020-03-31'\").groupby(\"RegionName\")\n\n#select the columns we want, and then use the built-in mean function\ncenter_avg_prices = center_group_avg[['ZHVI']].mean()\ncenter_avg_prices.round(2).sort_values(\"ZHVI\")\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19107\n329753.62\n\n\n19102\n350272.86\n\n\n19146\n383751.90\n\n\n19106\n403091.93\n\n\n19130\n415658.14\n\n\n19123\n434565.00\n\n\n19147\n449165.59\n\n\n19103\n485132.55\n\n\n\n\n\n\n\n\n\nCode\n#Calculate the average of ZVHI for outside of Center City\n#first trim to dates from March 2020 onwards, then group by county name\noutside_group_avg = phila_outside.query(\"Date &gt;= '2020-03-31'\").groupby(\"RegionName\")\n\n#select the columns we want, and then use the built-in mean function\noutside_avg_prices = outside_group_avg[['ZHVI']].mean()\noutside_avg_prices.round(2).sort_values(\"ZHVI\")\n\n\n\n\n\n\n\n\n\nZHVI\n\n\nRegionName\n\n\n\n\n\n19132\n72876.31\n\n\n19133\n79160.59\n\n\n19140\n85369.00\n\n\n19142\n112555.34\n\n\n19134\n113333.62\n\n\n19139\n136885.83\n\n\n19124\n146376.34\n\n\n19120\n150320.55\n\n\n19143\n163704.69\n\n\n19131\n164191.28\n\n\n19141\n170270.69\n\n\n19135\n177200.17\n\n\n19138\n180707.97\n\n\n19151\n188517.17\n\n\n19144\n192884.14\n\n\n19149\n201986.45\n\n\n19137\n203351.48\n\n\n19153\n207402.00\n\n\n19136\n209371.83\n\n\n19104\n212173.86\n\n\n19126\n213010.90\n\n\n19150\n223884.34\n\n\n19121\n228255.28\n\n\n19111\n241814.21\n\n\n19114\n260346.55\n\n\n19148\n264084.00\n\n\n19152\n266799.03\n\n\n19154\n268096.76\n\n\n19145\n268546.66\n\n\n19129\n279712.62\n\n\n19127\n292084.38\n\n\n19122\n294285.66\n\n\n19115\n302948.93\n\n\n19128\n309381.59\n\n\n19116\n317510.48\n\n\n19119\n327088.48\n\n\n19125\n328043.97\n\n\n19118\n681192.62",
    "crumbs": [
      "Analysis",
      "Assignment #1: The Donut Effect for Philadelphia ZIP Codes"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/Final_project.html",
    "href": "analysis/Final_project.html",
    "title": "Analysis",
    "section": "",
    "text": "Re-rethinking the causes of deforestation: does the theory evolve? by Desra Arriyadi"
  },
  {
    "objectID": "analysis/Final_project.html#missing-data",
    "href": "analysis/Final_project.html#missing-data",
    "title": "Analysis",
    "section": "Missing data",
    "text": "Missing data\nMissing values are handled using a threshold to remove columns or rows with excessive missing data.\nMissing values (NaN) in each column are counted and sorted in descending order to identify variables with the most missing data.\n\n\nCode\n# Display NaN data each column\nnan_counts = wbData.isna().sum().sort_values(ascending=False)\n\n\nColumns where missing values exceed half the total number of rows are removed (columns_to_drop), as they lack sufficient data for meaningful analysis.\n\n\nCode\n# Drop columns NaN\nhalfWbDataNan = len(wbData) / 2\ncolumns_to_drop = nan_counts[nan_counts &gt;= halfWbDataNan].index\nwbDataDropColumns = wbData.drop(columns=columns_to_drop)\nwbDataDropColumns.round(1)\n\n\n\n  \n    \n\n\n\n\n\n\nID\nCountry\nyear\nAG.LND.AGRI.K2\nAG.LND.FRST.ZS\nAG.LND.AGRI.ZS\nAG.LND.ARBL.HA\nAG.LND.CROP.ZS\nAG.LND.FRST.K2\nAG.PRD.CREL.MT\nAG.PRD.CROP.XD\nEN.POP.DNST\nGE.EST\nNV.AGR.TOTL.ZS\nNY.ADJ.DFOR.CD\nNY.GDP.FRST.RT.ZS\nNY.GDP.MKTP.KD.ZG\nPV.EST\nRQ.EST\nSE.PRM.ENRR\nSE.SEC.ENRR\nSL.AGR.EMPL.ZS\nSL.EMP.SELF.ZS\nSL.TLF.TOTL.IN\nSN.ITK.DEFC.ZS\nSP.POP.GROW\nSP.POP.TOTL\nSP.RUR.TOTL\nSP.RUR.TOTL.ZG\nSP.RUR.TOTL.ZS\nSP.URB.GROW\nSP.URB.TOTL.IN.ZS\nTM.VAL.AGRI.ZS.UN\nTM.VAL.MRCH.XD.WD\nTX.VAL.AGRI.ZS.UN\nBG.GSR.NFSV.GD.ZS\nBM.GSR.FCTY.CD\nBM.GSR.TRVL.ZS\nBX.GSR.GNFS.CD\nCC.EST\nDT.DOD.DECT.CD\nDT.DOD.DIMF.CD\nDT.TDS.DECT.CD\nDT.TDS.MLAT.CD\nFM.AST.DOMS.CN\nFM.AST.NFRG.CN\nFP.CPI.TOTL\nGC.TAX.GSRV.CN\nGC.XPN.TRFT.CN\nNY.GDP.MKTP.CD\n\n\n\n\n0\nABW\nAruba\n2000\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\n503.3\nNaN\n0.0\n13173.8\n0.0\n7.6\nNaN\nNaN\n108.3\n93.8\nNaN\nNaN\nNaN\nNaN\n1.0\n90588.0\n48268.0\n1.8\n53.3\n0.2\n46.7\n2.9\n206.5\n0.7\n119.0\n77016759.8\n22.6\n1.815620e+09\nNaN\nNaN\nNaN\nNaN\nNaN\n1.478961e+09\n5.541720e+08\n72.0\nNaN\nNaN\n1.873453e+09\n\n\n1\nABW\nAruba\n2001\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\n508.0\nNaN\n0.0\n13673.8\n0.0\n4.2\nNaN\nNaN\n110.9\n95.3\nNaN\nNaN\nNaN\nNaN\n0.9\n91439.0\n49067.0\n1.6\n53.7\n0.1\n46.3\n2.5\n189.7\n0.5\n128.6\n107083798.9\n21.9\n2.018207e+09\nNaN\nNaN\nNaN\nNaN\nNaN\n1.540655e+09\n7.006010e+08\n74.1\nNaN\nNaN\n1.896457e+09\n\n\n2\nABW\nAruba\n2002\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\n511.5\nNaN\n0.0\n12759.1\n0.0\n-0.9\nNaN\nNaN\n115.3\n100.3\nNaN\nNaN\nNaN\nNaN\n0.7\n92074.0\n49746.0\n1.4\n54.0\n-0.1\n46.0\n2.5\n161.3\n1.0\n93.7\n167910614.5\n26.0\n1.448782e+09\nNaN\nNaN\nNaN\nNaN\nNaN\n1.706137e+09\n7.469060e+08\n76.6\nNaN\nNaN\n1.961844e+09\n\n\n3\nABW\nAruba\n2003\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\n517.4\nNaN\n0.0\n14148.0\n0.0\n1.1\nNaN\nNaN\n113.9\n99.4\nNaN\nNaN\nNaN\nNaN\n1.1\n93128.0\n50656.0\n1.8\n54.4\n0.3\n45.6\n2.7\n191.7\n1.2\n114.2\n85882681.6\n25.8\n1.766140e+09\nNaN\nNaN\nNaN\nNaN\nNaN\n1.969561e+09\n7.009150e+08\n79.4\nNaN\nNaN\n2.044112e+09\n\n\n4\nABW\nAruba\n2004\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\n528.5\n1.3\n0.0\n15276.3\n0.0\n7.3\n1.0\n0.8\n116.2\n97.2\nNaN\nNaN\nNaN\nNaN\n2.1\n95138.0\n52098.0\n2.8\n54.8\n1.3\n45.2\n2.8\n287.0\n1.2\n84.2\n109927374.3\n33.8\n4.687899e+09\n1.2\nNaN\nNaN\nNaN\nNaN\n2.061564e+09\n7.227750e+08\n81.4\nNaN\nNaN\n2.254831e+09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5059\nZWE\nZimbabwe\n2019\n162000.0\n45.2\n39.5\n4000000.0\n0.1\n174906.5\n1026006.4\n104.3\n39.5\n-1.3\n9.8\n435782762.1\n2.0\n-6.3\n-0.9\n-1.5\n97.5\nNaN\n62.4\n70.9\n5772798.0\n39.4\n1.6\n15271368.0\n10352460.0\n1.6\n67.8\n1.6\n32.2\n0.4\n79.6\n2.2\n5.9\n354325147.1\n22.4\n5.266937e+09\n-1.3\n1.224936e+10\n4.681985e+08\n1.597240e+09\n16177509.3\n2.766927e+10\n-4.066235e+10\n414.7\nNaN\nNaN\n2.571741e+10\n\n\n5060\nZWE\nZimbabwe\n2020\n162000.0\n45.1\n39.8\n4000000.0\n0.2\n174445.8\n1660964.0\n127.8\n40.1\n-1.3\n8.8\n487068344.4\n2.3\n-7.8\n-1.1\n-1.4\n97.4\nNaN\n58.8\n67.8\n5842807.0\n39.5\n1.7\n15526888.0\n10520709.0\n1.6\n67.8\n1.8\n32.2\n0.3\n82.6\n1.7\n4.1\n480586954.8\n17.6\n5.263295e+09\n-1.3\n1.274203e+10\n4.876475e+08\n9.846859e+08\n9135529.1\n1.066966e+11\n-3.353351e+11\n2725.3\nNaN\nNaN\n2.686794e+10\n\n\n5061\nZWE\nZimbabwe\n2021\n162000.0\n45.0\n39.4\n4000000.0\n0.2\n173985.1\n2043436.4\n130.0\n40.8\n-1.3\n8.8\n516326002.5\n1.8\n8.5\n-1.0\n-1.4\n96.0\nNaN\n53.6\n64.2\n6005429.0\n38.9\n1.7\n15797210.0\n10694237.0\n1.6\n67.7\n1.9\n32.3\n0.3\n118.8\n2.2\n4.3\n662301359.7\n12.8\n6.574804e+09\n-1.3\n1.381758e+10\n1.422011e+09\n6.074121e+08\n20165936.9\n3.402869e+11\n-3.841839e+11\n5411.0\nNaN\nNaN\n2.724052e+10\n\n\n5062\nZWE\nZimbabwe\n2022\nNaN\n44.9\n39.5\nNaN\n0.2\n173524.4\n1887719.5\n123.5\n41.5\n-1.3\n7.2\nNaN\nNaN\n6.1\n-0.9\n-1.4\n95.8\nNaN\n52.6\n63.5\n6169164.0\n38.1\n1.7\n16069056.0\n10863485.0\n1.6\n67.6\n2.0\n32.4\n0.3\nNaN\n1.3\n5.8\n630683335.9\n15.0\n7.453497e+09\n-1.3\n1.382954e+10\n1.352160e+09\n4.542304e+08\n7842276.0\n1.887873e+12\n-2.301768e+12\n11076.6\nNaN\nNaN\n3.278975e+10\n\n\n5063\nZWE\nZimbabwe\n2023\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-1.2\n4.1\nNaN\nNaN\n5.3\n-0.9\n-1.3\nNaN\nNaN\nNaN\nNaN\n6320388.0\nNaN\n1.7\n16340822.0\n11027277.0\n1.5\n67.5\n2.1\n32.5\n0.3\nNaN\n1.2\n5.8\n442297465.1\n17.8\n7.602718e+09\n-1.3\n1.421339e+10\n1.363159e+09\n1.161955e+09\n62317585.3\n1.666135e+13\n-1.886184e+13\nNaN\nNaN\nNaN\n3.523137e+10\n\n\n\n\n5064 rows × 50 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nRows with any NaN values are dropped to retain only complete records for further analysis (rows_without_nan).\n\n\nCode\n# Select rows with no NaN values\nrows_without_nan = wbDataDropColumns.dropna(how='any')\n\n# Count rows for each country\ncountry_counts = rows_without_nan['Country'].value_counts()\n\n\nCounts the number of valid rows per country and filters countries with at least 7 valid entries, ensuring sufficient data coverage for each country.\n\n\nCode\n# Filter countries with at least 7 rows\nfiltered_countries = country_counts[country_counts &gt;= 7].index\n\n# Retain rows only for countries with at least 10 entries\npreProceedData = rows_without_nan[rows_without_nan['Country'].isin(filtered_countries)]\n\n\n\n\nCode\npreProceedData['Country'].value_counts().sort_values(ascending=True)\nprint(\"Total selected countries: \", len(preProceedData['Country'].value_counts()))\n\n\nTotal selected countries:  51\n\n\n\nVisualization of filtered countries\nCreate a geospatial visualization using folium to map filtered countries. The process starts by setting the coordinate reference system (CRS) of the GeoDataFrame (world) to WGS 84 for compatibility, followed by merging it with the cleaned dataset (preProceedData) based on country IDs. The merged data is then converted to GeoJSON format for mapping. A folium.Map is created with a dark background, and a GeoJson layer is added to highlight the filtered countries with a white fill color. The map includes zoom controls and interactive tooltips displaying country names for better exploration.\n\n\nCode\n# Set the CRS for the 'world' GeoDataFrame before merging\nworld = world.set_crs(\"epsg:4326\")  # Assuming the GeoJSON is in WGS 84\n\n# Merge the GeoDataFrame with Natural Earth GeoServer data\ngdf = world.merge(preProceedData, left_on='WB_A3', right_on='ID', how='inner')\n\n# Convert the GeoDataFrame to GeoJSON format\ngdf_json = gdf.to_crs(\"epsg:4326\").to_json()\n\n# Create a folium map\nm = folium.Map(\n    location=[30, 0],\n    zoom_start=2,\n    tiles='CartoDB dark_matter',\n    max_bounds=True,\n    max_zoom=10,\n    min_zoom=2\n)\n\n# Add a layer for filtered countries with white fill color\nfolium.GeoJson(\n    data=gdf_json,\n    style_function=lambda x: {\n        'fillColor': 'white',\n        'color': '#3F597C',\n        'weight': 0.5,\n        'fillOpacity': 0.7\n    },\n    tooltip=folium.GeoJsonTooltip(fields=['Country'], aliases=['Country:'])\n).add_to(m)\n\n# Display map\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/Final_project.html#dependent-variable",
    "href": "analysis/Final_project.html#dependent-variable",
    "title": "Analysis",
    "section": "Dependent variable",
    "text": "Dependent variable\nThe dependent variable, “Deforestation,” is computed as the year-on-year difference in forest area for each country and prepare it for further analysis and visualization.\nThe dataset is sorted by Country and year to ensure chronological consistency. The Deforestation variable is then calculated as the year-on-year difference in forest area (AG.LND.FRST.K2) for each country using the diff function, capturing annual changes in forest cover. Missing values from the calculation are handled by dropping incomplete rows. A copy of the processed data is created for visualization, and the regions are mapped to their full names for clarity. This dependent variable is essential for modeling deforestation trends and understanding the dynamics behind forest loss, while the cleaned and enriched data supports detailed visual and statistical analyses.\n\n\nCode\n# Reset index\npreProceedData2 = preProceedData.reset_index()\n\n# Ensure the data is sorted correctly\npreProceedData2 = preProceedData2.sort_values(by=['Country', 'year'])\n\n# Calculate the deforestation for each country by subtracting the forest area of the previous year\npreProceedData2['Deforestation'] = preProceedData2.groupby('Country')['AG.LND.FRST.K2'].diff()\n\n# For visualization\nforVis = preProceedData2.copy()\nforVis['Deforestation'] = forVis['Deforestation'].fillna(0)\n\n# Select rows with no NaN values\npreProceedData2 = preProceedData2.dropna(how='any')\n\n\n\nVisualization of deforestation\nEnrich the dataset for visualization by adding region information. It merges the data (forVis) with a filtered list of countries and their regions (allCountryFilt) based on country names. A new column, region_full, is added by mapping region abbreviations to their full names (e.g., “EAS” to “East Asia and Pacific”) using a predefined dictionary.\n\n\nCode\n#Additional region data for visualization\nallCountryFilt = allCountry[['name', 'region']]\n\nforVis2 = pd.merge(forVis, allCountryFilt, left_on=['Country'], right_on=['name'], how='inner')\n\n# Add a column to explain region abbreviations\nregion_mapping = {\n    \"EAS\": \"East Asia and Pacific\",\n    \"ECS\": \"Europe and Central Asia\",\n    \"LCN\": \"Latin America and Caribbean\",\n    \"MEA\": \"Middle East and North Africa\",\n    \"SAS\": \"South Asia\",\n    \"SSF\": \"Sub-Saharan Africa\"\n}\nforVis2['region_full'] = forVis2['region'].map(region_mapping)\n\n\nCreates an interactive stacked bar chart using Altair to visualize reported deforestation trends by region over the years. Custom colors are defined for each region, and a brush selection feature allows users to click on specific countries to highlight their data. The chart groups data by region_full and represents the sum of deforestation for each year, with a tooltip displaying details like Country, year, and region.\n\n\nCode\n#Define custom colors\ncustomColor = ['#7DC1DD','#EB9E42','#DF7068','#82C574','#82888D','#3F597C']\n\n#Create a point selection for the chart\nbrush = alt.selection_point(fields=['Country'], on='click')\n\n# Altair plot as a stacked bar chart\ndeforestation_chart = alt.Chart(forVis2).mark_bar(stroke='white', strokeWidth=0.5).encode(\n    x=alt.X('year:O', title='Year'),\n    y=alt.Y('sum(Deforestation):Q', title='Change in Forest Area, sq. km (Negative = forest loss)'),\n    color=alt.condition(\n        brush,\n        alt.Color(\"region_full:N\", scale=alt.Scale(range=customColor), legend=alt.Legend(title=\"Region\", orient=\"bottom-left\")),\n        alt.value('lightgray')\n    ),\n    tooltip=['Country', 'year', 'Deforestation', 'region', 'region_full']\n).add_params(\n    brush\n).properties(\n    title='Reported Deforestation',\n    width=800,\n    height=400\n)\n\n# Customize the legend position\nfinal_chart = deforestation_chart.configure_legend(\n    orient='bottom-left',\n    titleFontSize=12,\n    labelFontSize=10,\n    fillColor='white'\n)\n\n# Show the plot\nfinal_chart"
  },
  {
    "objectID": "analysis/Final_project.html#encoding-categorical",
    "href": "analysis/Final_project.html#encoding-categorical",
    "title": "Load data and preparation",
    "section": "Encoding categorical",
    "text": "Encoding categorical\n\n\nCode\n# # Perform train_test_split\n# X = dataForRF[[col for col in dataForRF.columns if col not in variableXExclude]]\n# y = dataForRF[\"Deforestation\"]\n\n\n\n\nCode\n# Split the data 70/30\ntrain_set, test_set = train_test_split(dataForRf, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# the target labels: log of Deforestation\ny_train = train_set[\"Deforestation\"]\ny_test = test_set[\"Deforestation\"]\n\n\n\n\nCode\ndropDefo = dataForRf.drop(columns=[\"Deforestation\"])\n\n# Divide data by type\nnum_cols = sorted(dropDefo.select_dtypes(include=['int64', 'float64']).columns.tolist())\ncat_cols = sorted(dropDefo.select_dtypes(include=['object']).columns.tolist())\n\nprint(f'There are {len(cat_cols)} categorical variables')\nprint(f'There are {len(num_cols)} numerical variables')\n\n\nThere are 3 categorical variables\nThere are 39 numerical variables\n\n\n\n\nCode\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\nX_train = train_set[num_cols + cat_cols]\nX_test = test_set[num_cols + cat_cols]\n\n\n\n\nCode\n# Initialize the pipeline\npipe = make_pipeline(\n    transformer, RandomForestRegressor(n_estimators=10,\n                                       random_state=42)\n)\n\n\n\n\nCode\n# Fit the training set\npipe.fit(train_set, y_train);\n\n\n\n\nCode\n# What's the test score?\npipe.score(test_set, y_test)\n\n\n0.6937210789584691\n\n\n\n\nCode\n# The one-hot step\nohe = transformer.named_transformers_['cat']\n\n# One column for each category type!\nohe_cols = ohe.get_feature_names_out()\n\n# Full list of columns is numerical + one-hot\nfeatures = num_cols + list(ohe_cols)\n\n\n\n\nCode\nrandom_forest = pipe[\"randomforestregressor\"]\n\n# Create the dataframe with importances\nimportance = pd.DataFrame(\n    {\"Feature\": features, \"Importance\": random_forest.feature_importances_}\n)\n\n\n\n\nCode\n# Sort the data and select the top 50 features\ntop_features = importance.sort_values(\"Importance\", ascending=False).round(5).head(20)\n\n# Create the Feature Importance plot using Altair\nfeature_importance_chart = alt.Chart(top_features).mark_bar().encode(\n    x=alt.X(\"Importance:Q\", title=\"Importance\"),\n    y=alt.Y(\"Feature:N\", sort='-x', title=\"Features\"),\n    tooltip=[\"Feature:N\", \"Importance:Q\"]\n).properties(\n    title=\"Top 20 Feature Importance Plot\",\n    width=600,\n    height=400\n).configure_title(\n    fontSize=16,\n    anchor='middle'\n)\n\nfeature_importance_chart.show()\n\n\n\n\n\n\n\n\nAmong all variables, the subsidies and other transfers (GC.XPN.TRFT.CN) is the most significant contributors to deforestation. By defintion, this variable benefits include all unrequited, nonrepayable transfers on current account to private and public enterprises; grants to foreign governments, international organizations, and other government units; and social security, social assistance benefits, and employer social benefits in cash and in kind.\nAccording to Amaglobeli et al. (2024), Agricultural producer subsidies are prevalent, large, and deployed to achieve diverse and, at times, overlapping policy objectives. Among countries accounting for 90 percent of global GDP, food and agriculture subsidies amount to 0.3–0.7 percent of GDP over the past decade and a half.\nAs highlighted by Damania et al. (2023), highlighted inefficiency of agricultural and timber subsidies. Between 2016 and 2018, $635 billion per year (equals approximately 0.9 percent of GDP and nearly one-fifth of agricul- tural value added for these countries) was given as support to agriculture in 84 countries. 71% of this support went directly to farmers or producers, mainly in ways that encouraged them to produce more or use certain inputs, which can influence their decisions on what and how much to produce. This type of support often encourage unsustainable practices, such as excessive use of chemical fertilizers and pesticides, which lead to greenhouse gas emissions, land degradation, and biodiversity loss. Similarly, timber subsidies contribute to overharvesting, illegal logging, and forest degradation. They distort markets by making sustainable forest management less competitive than subsidized, unsustainable logging.\n\n\nCode\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    pipe,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.85165448 0.53253658 0.48310187]\nScores mean =  0.6224309745819587\nScore std dev =  0.16333709037439043"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Question",
    "section": "",
    "text": "Why does the number of acres of forest loss keep counting even when I reload the page? Why doesn’t it reset to 0?\nThe forest loss counter stores its data in your browser’s local storage. If you want to reset it to 0, you can clear your browser cache. Remember that stopping deforestation requires consistent efforts—just like clearing your cache requires action on your part."
  },
  {
    "objectID": "analysis/Final_project.html#environment-setup",
    "href": "analysis/Final_project.html#environment-setup",
    "title": "Analysis",
    "section": "Environment setup",
    "text": "Environment setup\nThis section sets up the Python environment by importing essential libraries like pandas, numpy, sklearn, and other libraries.\nPackages like wbgapi (for accessing World Bank data) and geoviews (for geospatial visualization) are installed to handle data processing and visualization tasks.\n\n\nCode\n%%capture\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\n\nCode\n%%capture\n%cd /content/drive/MyDrive/Colab Notebooks\n\n\n\n\nCode\n!pip install --quiet wbgapi\n!pip install --quiet hvplot\n!pip install --quiet geoviews cartopy\n\n\n\n\nCode\nimport wbgapi as wb\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nimport altair as alt\nimport requests\nimport folium\nfrom folium import Choropleth\nimport holoviews as hv\nimport hvplot.pandas\nfrom vega_datasets import data\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n\n# Pre-Processing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Pipelines\nfrom sklearn.pipeline import make_pipeline\n\n# Metrics\nfrom sklearn.metrics import recall_score, r2_score\n\npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows', 100)\nhv.extension(\"bokeh\")"
  },
  {
    "objectID": "analysis/Final_project.html#data-loading",
    "href": "analysis/Final_project.html#data-loading",
    "title": "Analysis",
    "section": "Data loading",
    "text": "Data loading\nFilter out non-country entities (like regions or continents) from the World Bank data. The allCountry DataFrame initially includes all entities, but only rows with valid country-level data are retained by dropping NaN values and extracting the country id into allCountryList.\n\n\nCode\n# Select only ID countries by excluding region/group/etc\nallCountry = wb.economy.DataFrame()\nallCountryList = allCountry.dropna().reset_index()['id'].tolist()\n\n\nData is retrieved for specific variables relevant to deforestation, such as forest area, agricultural land, economy, and other variables.\n\n\nCode\nallVariables =['AG.LND.AGRI.K2',\n'AG.LND.FRST.ZS',\n'AG.LND.AGRI.ZS',\n'AG.LND.ARBL.HA',\n'AG.LND.CROP.ZS',\n'AG.LND.FRST.K2',\n'AG.LND.TOTL.UR.K2',\n'AG.PRD.CREL.MT',\n'AG.PRD.CROP.XD',\n'EG.USE.COMM.FO.ZS',\n'EG.USE.PCAP.KG.OE',\n'EN.POP.DNST',\n'GE.EST',\n'NV.AGR.TOTL.ZS',\n'NY.ADJ.DFOR.CD',\n'NY.GDP.FRST.RT.ZS',\n'NY.GDP.MKTP.KD.ZG',\n'PV.EST',\n'RQ.EST',\n'SE.ADT.LITR.ZS',\n'SE.PRM.ENRR',\n'SE.SEC.ENRR',\n'SH.STA.MALN.ZS',\n'SI.POV.DDAY',\n'SI.POV.GINI',\n'SL.AGR.0714.ZS',\n'SL.AGR.EMPL.ZS',\n'SL.EMP.SELF.ZS',\n'SL.TLF.TOTL.IN',\n'SN.ITK.DEFC.ZS',\n'SP.POP.GROW',\n'SP.POP.TOTL',\n'SP.RUR.TOTL',\n'SP.RUR.TOTL.ZG',\n'SP.RUR.TOTL.ZS',\n'SP.URB.GROW',\n'SP.URB.TOTL.IN.ZS',\n'TM.VAL.AGRI.ZS.UN',\n'TM.VAL.MRCH.XD.WD',\n'TX.VAL.AGRI.ZS.UN',\n'VC.IDP.TOCV',\n'AG.CON.FERT.PT.ZS',\n'AG.LND.TOTL.RU.K2',\n'BG.GSR.NFSV.GD.ZS',\n'BM.GSR.FCTY.CD',\n'BM.GSR.TRVL.ZS',\n'BX.GSR.GNFS.CD',\n'CC.EST',\n'CM.MKT.LCAP.CD',\n'DT.DOD.DECT.CD',\n'DT.DOD.DIMF.CD',\n'DT.TDS.DECT.CD',\n'DT.TDS.MLAT.CD',\n'FB.CBK.BRWR.P3',\n'FM.AST.DOMS.CN',\n'FM.AST.NFRG.CN',\n'FP.CPI.TOTL',\n'GC.TAX.EXPT.CN',\n'GC.TAX.GSRV.CN',\n'GC.TAX.INTT.CN',\n'GC.XPN.TRFT.CN',\n'HD.HCI.OVRL',\n'IS.RRS.TOTL.KM',\n'LP.EXP.DURS.MD',\n'NY.GDP.MKTP.CD'\n]\n\n\nThis function process_wb_variables retrieves and processes data from the World Bank API for a list of variables, countries, and years.\nPurpose: Fetch data for specified variables and merge into a single DataFrame for analysis. Process includes data for each variable is fetched and cleaned (resetting index, renaming columns, filtering by countries, and restructuring into long format). Years are standardized as integers. Each variable’s data is stored in a list. All variables’ data are merged into a single DataFrame by country, year, and ID. Error Handling: Logs errors for individual variables and raises an exception if no data is successfully processed. Output: Returns a comprehensive DataFrame (wbData) containing aligned data for all variables, countries, and years.\n\n\nCode\n# Note: range start year to end year-1\ndef process_wb_variables(variables, allCountryList, time_range=range(2000, 2024)):\n    # List to store processed dataframes\n    processed_dfs = []\n\n    # Process each variable\n    for variable in variables:\n        try:\n            # Fetch data\n            dfGetData = wb.data.DataFrame(variable, time=time_range, labels=True)\n\n            # Reset the index to make 'economy' and 'economy_label' as columns\n            dfGetData = dfGetData.reset_index()\n\n            # Rename the columns\n            dfGetData = dfGetData.rename(columns={\n                'economy': 'ID',\n                dfGetData.columns[1]: 'Country'\n            })\n\n            # Filter countries\n            dfGetData = dfGetData[dfGetData['ID'].isin(allCountryList)]\n\n            # Melt the DataFrame\n            melted_df = dfGetData.melt(\n                id_vars=['ID', 'Country'],\n                var_name='year',\n                value_name=variable  # Use variable code as value column name\n            )\n\n            # Convert year to integer\n            melted_df['year'] = melted_df['year'].astype(str).str.replace('YR', '').astype(int)\n\n            # Sort and add to list\n            melted_df = melted_df.sort_values(['ID', 'year'])\n\n            processed_dfs.append(melted_df)\n\n            #print(f\"Processed {variable} successfully\")\n\n        except Exception as e:\n            print(f\"Error processing {variable}: {e}\")\n\n    # Merge all processed dataframes\n    if processed_dfs:\n        # Merge on ID, Country, and year\n        merged_df = processed_dfs[0]\n        for dfGetData in processed_dfs[1:]:\n            merged_df = pd.merge(merged_df, dfGetData, on=['ID', 'Country', 'year'], how='outer')\n\n        return merged_df\n    else:\n        raise ValueError(\"No variables could be processed\")\n\n# Process all variables\nwbData = process_wb_variables(allVariables, allCountryList)\n\n\n\n\nCode\nwbData.round(1)\n\n\n\n  \n    \n\n\n\n\n\n\nID\nCountry\nyear\nAG.LND.AGRI.K2\nAG.LND.FRST.ZS\nAG.LND.AGRI.ZS\nAG.LND.ARBL.HA\nAG.LND.CROP.ZS\nAG.LND.FRST.K2\nAG.LND.TOTL.UR.K2\nAG.PRD.CREL.MT\nAG.PRD.CROP.XD\nEG.USE.COMM.FO.ZS\nEG.USE.PCAP.KG.OE\nEN.POP.DNST\nGE.EST\nNV.AGR.TOTL.ZS\nNY.ADJ.DFOR.CD\nNY.GDP.FRST.RT.ZS\nNY.GDP.MKTP.KD.ZG\nPV.EST\nRQ.EST\nSE.ADT.LITR.ZS\nSE.PRM.ENRR\nSE.SEC.ENRR\nSH.STA.MALN.ZS\nSI.POV.DDAY\nSI.POV.GINI\nSL.AGR.0714.ZS\nSL.AGR.EMPL.ZS\nSL.EMP.SELF.ZS\nSL.TLF.TOTL.IN\nSN.ITK.DEFC.ZS\nSP.POP.GROW\nSP.POP.TOTL\nSP.RUR.TOTL\nSP.RUR.TOTL.ZG\nSP.RUR.TOTL.ZS\nSP.URB.GROW\nSP.URB.TOTL.IN.ZS\nTM.VAL.AGRI.ZS.UN\nTM.VAL.MRCH.XD.WD\nTX.VAL.AGRI.ZS.UN\nVC.IDP.TOCV\nAG.CON.FERT.PT.ZS\nAG.LND.TOTL.RU.K2\nBG.GSR.NFSV.GD.ZS\nBM.GSR.FCTY.CD\nBM.GSR.TRVL.ZS\nBX.GSR.GNFS.CD\nCC.EST\nCM.MKT.LCAP.CD\nDT.DOD.DECT.CD\nDT.DOD.DIMF.CD\nDT.TDS.DECT.CD\nDT.TDS.MLAT.CD\nFB.CBK.BRWR.P3\nFM.AST.DOMS.CN\nFM.AST.NFRG.CN\nFP.CPI.TOTL\nGC.TAX.EXPT.CN\nGC.TAX.GSRV.CN\nGC.TAX.INTT.CN\nGC.XPN.TRFT.CN\nHD.HCI.OVRL\nIS.RRS.TOTL.KM\nLP.EXP.DURS.MD\nNY.GDP.MKTP.CD\n\n\n\n\n0\nABW\nAruba\n2000\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\n87.4\nNaN\nNaN\nNaN\nNaN\n503.3\nNaN\n0.0\n13173.8\n0.0\n7.6\nNaN\nNaN\n97.0\n108.3\n93.8\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n90588.0\n48268.0\n1.8\n53.3\n0.2\n46.7\n2.9\n206.5\n0.7\nNaN\nNaN\n94.8\n119.0\n77016759.8\n22.6\n1.815620e+09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.478961e+09\n5.541720e+08\n72.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.873453e+09\n\n\n1\nABW\nAruba\n2001\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\nNaN\nNaN\nNaN\n508.0\nNaN\n0.0\n13673.8\n0.0\n4.2\nNaN\nNaN\nNaN\n110.9\n95.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.9\n91439.0\n49067.0\n1.6\n53.7\n0.1\n46.3\n2.5\n189.7\n0.5\nNaN\nNaN\nNaN\n128.6\n107083798.9\n21.9\n2.018207e+09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.540655e+09\n7.006010e+08\n74.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.896457e+09\n\n\n2\nABW\nAruba\n2002\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\nNaN\nNaN\nNaN\n511.5\nNaN\n0.0\n12759.1\n0.0\n-0.9\nNaN\nNaN\nNaN\n115.3\n100.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.7\n92074.0\n49746.0\n1.4\n54.0\n-0.1\n46.0\n2.5\n161.3\n1.0\nNaN\nNaN\nNaN\n93.7\n167910614.5\n26.0\n1.448782e+09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.706137e+09\n7.469060e+08\n76.6\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.961844e+09\n\n\n3\nABW\nAruba\n2003\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\nNaN\nNaN\nNaN\n517.4\nNaN\n0.0\n14148.0\n0.0\n1.1\nNaN\nNaN\nNaN\n113.9\n99.4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.1\n93128.0\n50656.0\n1.8\n54.4\n0.3\n45.6\n2.7\n191.7\n1.2\nNaN\nNaN\nNaN\n114.2\n85882681.6\n25.8\n1.766140e+09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.969561e+09\n7.009150e+08\n79.4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.044112e+09\n\n\n4\nABW\nAruba\n2004\n20.0\n2.3\n11.1\n2000.0\nNaN\n4.2\nNaN\nNaN\nNaN\nNaN\nNaN\n528.5\n1.3\n0.0\n15276.3\n0.0\n7.3\n1.0\n0.8\nNaN\n116.2\n97.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.1\n95138.0\n52098.0\n2.8\n54.8\n1.3\n45.2\n2.8\n287.0\n1.2\nNaN\nNaN\nNaN\n84.2\n109927374.3\n33.8\n4.687899e+09\n1.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.061564e+09\n7.227750e+08\n81.4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.254831e+09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5059\nZWE\nZimbabwe\n2019\n162000.0\n45.2\n39.5\n4000000.0\n0.1\n174906.5\nNaN\n1026006.4\n104.3\nNaN\nNaN\n39.5\n-1.3\n9.8\n435782762.1\n2.0\n-6.3\n-0.9\n-1.5\n93.2\n97.5\nNaN\n9.7\n39.8\n50.3\nNaN\n62.4\n70.9\n5772798.0\n39.4\n1.6\n15271368.0\n10352460.0\n1.6\n67.8\n1.6\n32.2\n0.4\n79.6\n2.2\nNaN\n623.5\nNaN\n5.9\n354325147.1\n22.4\n5.266937e+09\n-1.3\nNaN\n1.224936e+10\n4.681985e+08\n1.597240e+09\n16177509.3\n85.1\n2.766927e+10\n-4.066235e+10\n414.7\nNaN\nNaN\nNaN\nNaN\nNaN\n3120.0\nNaN\n2.571741e+10\n\n\n5060\nZWE\nZimbabwe\n2020\n162000.0\n45.1\n39.8\n4000000.0\n0.2\n174445.8\nNaN\n1660964.0\n127.8\nNaN\nNaN\n40.1\n-1.3\n8.8\n487068344.4\n2.3\n-7.8\n-1.1\n-1.4\nNaN\n97.4\nNaN\nNaN\nNaN\nNaN\nNaN\n58.8\n67.8\n5842807.0\n39.5\n1.7\n15526888.0\n10520709.0\n1.6\n67.8\n1.8\n32.2\n0.3\n82.6\n1.7\nNaN\n212.5\nNaN\n4.1\n480586954.8\n17.6\n5.263295e+09\n-1.3\nNaN\n1.274203e+10\n4.876475e+08\n9.846859e+08\n9135529.1\n54.1\n1.066966e+11\n-3.353351e+11\n2725.3\nNaN\nNaN\nNaN\nNaN\n0.5\n3120.0\nNaN\n2.686794e+10\n\n\n5061\nZWE\nZimbabwe\n2021\n162000.0\n45.0\n39.4\n4000000.0\n0.2\n173985.1\nNaN\n2043436.4\n130.0\nNaN\nNaN\n40.8\n-1.3\n8.8\n516326002.5\n1.8\n8.5\n-1.0\n-1.4\nNaN\n96.0\nNaN\nNaN\nNaN\nNaN\nNaN\n53.6\n64.2\n6005429.0\n38.9\n1.7\n15797210.0\n10694237.0\n1.6\n67.7\n1.9\n32.3\n0.3\n118.8\n2.2\nNaN\n212.5\nNaN\n4.3\n662301359.7\n12.8\n6.574804e+09\n-1.3\nNaN\n1.381758e+10\n1.422011e+09\n6.074121e+08\n20165936.9\n61.5\n3.402869e+11\n-3.841839e+11\n5411.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.724052e+10\n\n\n5062\nZWE\nZimbabwe\n2022\nNaN\n44.9\n39.5\nNaN\n0.2\n173524.4\nNaN\n1887719.5\n123.5\nNaN\nNaN\n41.5\n-1.3\n7.2\nNaN\nNaN\n6.1\n-0.9\n-1.4\n89.8\n95.8\nNaN\nNaN\nNaN\nNaN\nNaN\n52.6\n63.5\n6169164.0\n38.1\n1.7\n16069056.0\n10863485.0\n1.6\n67.6\n2.0\n32.4\n0.3\nNaN\n1.3\nNaN\n212.5\nNaN\n5.8\n630683335.9\n15.0\n7.453497e+09\n-1.3\nNaN\n1.382954e+10\n1.352160e+09\n4.542304e+08\n7842276.0\n68.4\n1.887873e+12\n-2.301768e+12\n11076.6\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n3.278975e+10\n\n\n5063\nZWE\nZimbabwe\n2023\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-1.2\n4.1\nNaN\nNaN\n5.3\n-0.9\n-1.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n6320388.0\nNaN\n1.7\n16340822.0\n11027277.0\n1.5\n67.5\n2.1\n32.5\n0.3\nNaN\n1.2\nNaN\nNaN\nNaN\n5.8\n442297465.1\n17.8\n7.602718e+09\n-1.3\nNaN\n1.421339e+10\n1.363159e+09\n1.161955e+09\n62317585.3\n74.5\n1.666135e+13\n-1.886184e+13\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n3.523137e+10\n\n\n\n\n5064 rows × 68 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nCreate a DataFrame codeDescription that maps World Bank variable codes to their human-readable descriptions, making the data easier to interpret. The DataFrame contains two columns: Variable (the code) and Description (its explanation).\nThe output of this cell is referenced multiple times in subsequent processes to provide clear, descriptive labels for the variables used in the analysis.\n\n\nCode\ncodeDescription = pd.DataFrame(wb.series.info().table(), columns=['Variable', 'Description'])\ncodeDescription\n\n\n\n  \n    \n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\n0\nAG.CON.FERT.PT.ZS\nFertilizer consumption (% of fertilizer produc...\n\n\n1\nAG.CON.FERT.ZS\nFertilizer consumption (kilograms per hectare ...\n\n\n2\nAG.LND.AGRI.K2\nAgricultural land (sq. km)\n\n\n3\nAG.LND.AGRI.ZS\nAgricultural land (% of land area)\n\n\n4\nAG.LND.ARBL.HA\nArable land (hectares)\n\n\n...\n...\n...\n\n\n1492\nVC.IDP.TOCV\nInternally displaced persons, total displaced ...\n\n\n1493\nVC.IHR.PSRC.FE.P5\nIntentional homicides, female (per 100,000 fem...\n\n\n1494\nVC.IHR.PSRC.MA.P5\nIntentional homicides, male (per 100,000 male)\n\n\n1495\nVC.IHR.PSRC.P5\nIntentional homicides (per 100,000 people)\n\n\n1496\n\n1496 elements\n\n\n\n\n1497 rows × 2 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nVisualization of all World Bank Data\nReshape the dataset from wide to long format using pd.melt, where variables are listed under a Variable column, and their corresponding values are placed in a Value column. Essential identifiers like ID, Country, and year are retained, while all other columns are melted. The reshaped data is then grouped by ID, Country, and Variable to calculate the average value of each variable across all years, resulting in two DataFrames: wbData_melted for detailed analysis and wbAverageValue for summarizing variable averages by country.\n\n\nCode\ncolumns_to_keep = [\"ID\", \"Country\", \"year\"]\ndata_columns = wbData.columns.difference(columns_to_keep)\n\n# Use pd.melt to reshape the data\nwbData_melted = pd.melt(\n    wbData,\n    id_vars=columns_to_keep,\n    value_vars=data_columns,\n    var_name=\"Variable\",\n    value_name=\"Value\"\n)\n\n# Calculate the average Value based on year\nwbAverageValue = wbData_melted.groupby(['ID','Country', 'Variable'])['Value'].mean().reset_index()\n\n\nFetch Country Boundaries: A GeoJSON file containing country boundary data is retrieved from the Natural Earth GeoServer. The requests library is used to fetch the data, and it is converted into a GeoDataFrame (world) using geopandas.\n\n\nCode\n# Get country boundaries from Natural Earth GeoServer\nurl = \"https://raw.githubusercontent.com/nvkelso/natural-earth-vector/master/geojson/ne_110m_admin_0_countries.geojson\"\nresponse = requests.get(url, headers={\"Accept\": \"application/json\"})\ngeojson_data = response.json()\nworld = gpd.GeoDataFrame.from_features(geojson_data[\"features\"])\n\n\nThe GeoDataFrame (world) is merged with the previously cleaned World Bank data (wbAverageValue) based on the matching country IDs (WB_A3 from the GeoServer and ID from the cleaned data). This creates a GeoDataFrame (wbDataGeo) that combines spatial and non-spatial data for each country. The merged GeoDataFrame retains only necessary columns: geometry (spatial information for mapping), Country, Variable (World Bank variable), and Value (its average value).\n\n\nCode\n# Merge the GeoDataFrame with Natural Earth GeoServer data\nwbDataGeo = world.merge(wbAverageValue, left_on='WB_A3', right_on='ID', how='inner')\n\n# Keep only the geometry column\nwbDataGeo = wbDataGeo[[\"geometry\", \"Country\", \"Variable\", \"Value\"]]\n\n\nA copy of the merged GeoDataFrame (wbDataGeoCopy) is created and enriched by merging with the codeDescription DataFrame to add human-readable descriptions for each variable. Values are rounded for clearer presentation.\n\n\nCode\n# Create dataframe for visualization\nwbDataGeoCopy = wbDataGeo.copy()\nwbDataGeoVis = pd.merge(wbDataGeoCopy, codeDescription, on='Variable')\nwbDataGeoVis['Value'] = wbDataGeoVis['Value'].round(2)\nwbDataGeoVis.round(1)\n\n\n\n  \n    \n\n\n\n\n\n\ngeometry\nCountry\nVariable\nValue\nDescription\n\n\n\n\n0\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\nFiji\nAG.CON.FERT.PT.ZS\nNaN\nFertilizer consumption (% of fertilizer produc...\n\n\n1\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\nFiji\nAG.LND.AGRI.K2\n3290.9\nAgricultural land (sq. km)\n\n\n2\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\nFiji\nAG.LND.AGRI.ZS\n18.0\nAgricultural land (% of land area)\n\n\n3\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\nFiji\nAG.LND.ARBL.HA\n79263.6\nArable land (hectares)\n\n\n4\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\nFiji\nAG.LND.CROP.ZS\n4.2\nPermanent cropland (% of land area)\n\n\n...\n...\n...\n...\n...\n...\n\n\n10655\nPOLYGON ((30.83385 3.50917, 29.9535 4.1737, 29...\nSouth Sudan\nSP.URB.TOTL.IN.ZS\n18.4\nUrban population (% of total population)\n\n\n10656\nPOLYGON ((30.83385 3.50917, 29.9535 4.1737, 29...\nSouth Sudan\nTM.VAL.AGRI.ZS.UN\nNaN\nAgricultural raw materials imports (% of merch...\n\n\n10657\nPOLYGON ((30.83385 3.50917, 29.9535 4.1737, 29...\nSouth Sudan\nTM.VAL.MRCH.XD.WD\nNaN\nImport value index (2015 = 100)\n\n\n10658\nPOLYGON ((30.83385 3.50917, 29.9535 4.1737, 29...\nSouth Sudan\nTX.VAL.AGRI.ZS.UN\nNaN\nAgricultural raw materials exports (% of merch...\n\n\n10659\nPOLYGON ((30.83385 3.50917, 29.9535 4.1737, 29...\nSouth Sudan\nVC.IDP.TOCV\n1280461.5\nInternally displaced persons, total displaced ...\n\n\n\n\n10660 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nCreate an interactive geospatial map using hvplot to visualize World Bank data across countries. The map displays variable values (Value) with color gradients (cmap=‘plasma’) on a dark background (tiles=‘CartoDark’) and allows users to switch between variables using a dropdown menu (groupby=‘Description’). Hover functionality reveals country names and values, and the map is styled with a minimal dark theme for clarity.\n\n\nCode\n# Boundary\nx_rangemap = (-180, 180)\ny_rangemap = (-90, 90)\n\n# Create map\nimg = wbDataGeoVis.hvplot(\n    geo=True,\n    dynamic=False,\n    tiles='CartoDark',\n    frame_width=400,\n    frame_height=400,\n    c='Value',\n    groupby='Description',\n    cmap='plasma',\n    hover_cols=['Country', 'Value'],\n    xlim=x_rangemap,\n    ylim=y_rangemap\n)\n\nhv.renderer('bokeh').theme = 'dark_minimal'\nimg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nVisualization of Missing Data:\nGenerate a heatmap to visualize missing values in the dataset, sorted by columns and countries. It calculates the number of NaN values per column and country, reorders the data accordingly, and creates a binary DataFrame (NaN vs. non-NaN) for the heatmap. The plot uses color coding (blue for NaN and green for valid data) to highlight patterns of missing data, helping in understanding the extent of missing data and deciding on the cleaning strategy.\n\n\nCode\n# Recalculate the number of NaN values per column and per country\nhmColumnSort = wbData.isna().sum().sort_values(ascending=False)\nhmCountrySort = wbData.set_index(['ID', 'Country']).isna().sum(axis=1).sort_values(ascending=False)\n\n# Reorder the DataFrame based on sorted NaN counts\nhmSort = wbData[hmColumnSort.index]\n\n# Reorder the countries based on their NaN counts\nhmSort = hmSort.set_index(['ID', 'Country']).loc[hmCountrySort.index]\n\n# Create a DataFrame indicating NaN values for the heatmap\nhmBothSort = hmSort.isna()\n\n# Plotting the heatmap\nplt.figure(figsize=(20, 10))\nsns.heatmap(\n    hmBothSort.T,\n    cmap=['#82C574', '#3F597C'],\n    cbar=True,\n    cbar_kws={'label': 'Blue = NaN, Green = Not NaN'}\n)\n\nplt.title('Heatmap of Missing Values (Sorted by Columns and Countries)', fontsize=16)\nplt.xlabel('Country (ID)', fontsize=12)\nplt.ylabel('Columns (Variables)', fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "analysis/Final_project.html#independent-variables",
    "href": "analysis/Final_project.html#independent-variables",
    "title": "Analysis",
    "section": "Independent variables",
    "text": "Independent variables\nA correlation matrix is generated to identify variables that are highly correlated which will be excluded to reduce redundancy and improve model interpretability. This code calculates the correlation matrix for numerical variables in the dataset to identify relationships between them.\nA mask is created to cover the upper triangle of the matrix, leaving only the lower triangle visible for analysis (for distribution graph only)\n\n\nCode\n# Compute the correlation matrix\ncorr = preProceedData2.select_dtypes(include=['number']).corr()\n\n# Create a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Extract the values from the lower triangle of the correlation matrix\nlower_triangle_values = corr.where(~mask).stack().values\n\n\n\nVisualization of correlation matrix\nVisualize the correlation matrix and its distribution using side-by-side plots. A custom color gradient is defined for the heatmap to highlight correlations, which is plotted in the first subplot to show relationships between numerical variables. The second subplot displays a histogram of the lower triangle correlation values, with a density curve (kde) overlaid to illustrate the distribution of correlation strengths. Together, these plots help identify patterns and potential multicollinearity in the dataset.\n\n\nCode\n# Define a custom color gradient\ncustom_cmap = LinearSegmentedColormap.from_list(\n    \"custom_gradient\",\n    [\"#82C574\", \"#FFFFFF\", \"#DF7068\"]\n)\n\n# Create side-by-side plots\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# Plot the heatmap on the first subplot\nsns.heatmap(\n    preProceedData2.select_dtypes(include=['number']).corr(),\n    cmap=custom_cmap,\n    annot=False,\n    vmin=-1,\n    vmax=1,\n    #mask=mask,\n    ax=axes[0]\n)\naxes[0].set_title('Correlation Matrix Heatmap', fontsize=16)\n\n# Plot the distribution on the second subplot\nsns.histplot(\n    lower_triangle_values,\n    bins=30,\n    kde=True,\n    color='#7DC1DD',\n    ax=axes[1]\n)\naxes[1].set_title('Distribution of Lower Triangle Correlation Values', fontsize=16)\naxes[1].set_xlabel('Correlation Value', fontsize=12)\naxes[1].set_ylabel('Frequency', fontsize=12)\naxes[1].grid(True)\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIdentify pairs of variables with high correlations (≥ 0.9 or ≤ -0.9), which indicate potential multicollinearity. Using the masked lower triangle of the correlation matrix, it iterates through variable pairs to extract those meeting the threshold. These pairs, along with their correlation values, are stored in a list and converted into a DataFrame (high_correl) for better visualization and analysis. This helps in pinpointing strongly correlated variables that may need to be excluded or handled in modeling.\n\n\nCode\n# Identify correlations &gt;= 0.9 or &lt;= -0.9\nhigh_corr = corr[(corr &gt;= 0.9) | (corr &lt;= -0.9)]\n\n# Get the column names of high_corr\nhigh_corr_columns = high_corr.index\n\n# Apply the mask to keep only the lower triangle\ncorr_masked = corr.where(~mask)\n\n# Identify correlations &gt;= 0.9 or &lt;= -0.9 in the lower triangle\nhigh_corr_pairs = []\nfor i in corr_masked.index:\n    for j in corr_masked.columns:\n        if not pd.isna(corr_masked.loc[i, j]) and ((corr_masked.loc[i, j] &gt;= 0.9) or (corr_masked.loc[i, j] &lt;= -0.9)):\n            high_corr_pairs.append((i, j, corr_masked.loc[i, j]))\n\n# Convert to a DataFrame for better visualization\nhigh_correl = pd.DataFrame(high_corr_pairs, columns=['Variable 1', 'Variable 2', 'Correlation'])\n\n\nEnriches the highly correlated variable pairs (high_correl) by merging them with the codeDescription DataFrame to replace variable codes with their human-readable descriptions. Each variable pair is processed separately, renaming and organizing columns for clarity. The final DataFrame, high_corr_df, presents the pairs with their descriptive names and correlation values.\n\n\nCode\n# Merge the two dataframes for variable 1\nhigh_corr_var1 = pd.merge(\n    high_correl,\n    codeDescription,\n    left_on='Variable 1',\n    right_on='Variable',\n    how='left'\n)\n\nhigh_corr_var1 = high_corr_var1.drop(columns=['Variable','Variable 1'])\n\n# Rename a column in the dataframe\nhigh_corr_var1.rename(columns={'Description': 'Variable 1'}, inplace=True)\n\n# Merge the two dataframes for variable 2\nhigh_corr_df = pd.merge(\n    high_corr_var1,\n    codeDescription,\n    left_on='Variable 2',\n    right_on='Variable',\n    how='left'\n)\n\nhigh_corr_df = high_corr_df.drop(columns=['Variable','Variable 2'])\n\n# Rename a column in the dataframe\nhigh_corr_df.rename(columns={'Description': 'Variable 2'}, inplace=True)\n\nhigh_corr_df = high_corr_df[['Variable 1', 'Variable 2', 'Correlation']]\n\n# Display the result\nhigh_corr_df.round(2)\n\n\n\n  \n    \n\n\n\n\n\n\nVariable 1\nVariable 2\nCorrelation\n\n\n\n\n0\nCereal production (metric tons)\nArable land (hectares)\n0.97\n\n\n1\nSelf-employed, total (% of total employment) (...\nEmployment in agriculture (% of total employme...\n0.90\n\n\n2\nLabor force, total\nArable land (hectares)\n0.94\n\n\n3\nLabor force, total\nCereal production (metric tons)\n0.96\n\n\n4\nPopulation, total\nArable land (hectares)\n0.94\n\n\n5\nPopulation, total\nCereal production (metric tons)\n0.95\n\n\n6\nPopulation, total\nLabor force, total\n1.00\n\n\n7\nRural population\nArable land (hectares)\n0.91\n\n\n8\nRural population\nCereal production (metric tons)\n0.90\n\n\n9\nRural population\nLabor force, total\n0.97\n\n\n10\nRural population\nPopulation, total\n0.99\n\n\n11\nUrban population growth (annual %)\nPopulation growth (annual %)\n0.91\n\n\n12\nUrban population (% of total population)\nRural population (% of total population)\n-1.00\n\n\n13\nDebt service on external debt, total (TDS, cur...\nExternal debt stocks, total (DOD, current US$)\n0.93\n\n\n14\nNet foreign assets (current LCU)\nNet domestic credit (current LCU)\n0.99\n\n\n15\nTaxes on goods and services (current LCU)\nNet domestic credit (current LCU)\n0.99\n\n\n16\nTaxes on goods and services (current LCU)\nNet foreign assets (current LCU)\n0.99\n\n\n17\nSubsidies and other transfers (current LCU)\nNet domestic credit (current LCU)\n0.98\n\n\n18\nSubsidies and other transfers (current LCU)\nNet foreign assets (current LCU)\n0.99\n\n\n19\nSubsidies and other transfers (current LCU)\nTaxes on goods and services (current LCU)\n0.99\n\n\n20\nGDP (current US$)\nExternal debt stocks, total (DOD, current US$)\n0.93\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAggregate the variables from highly correlated pairs to identify which variables are most frequently involved in strong correlations. It combines Variable 1 and Variable 2 into a single series, counts their occurrences, and converts the result into a DataFrame (variable_counts_df) for better visualization. This highlights variables that are repeatedly involved in multicollinearity, helping to prioritize them for further investigation or potential exclusion from the analysis.\n\n\nCode\n# Combine Variable 1 and Variable 2 into a single series\nall_variables = pd.concat([high_corr_df['Variable 1'], high_corr_df['Variable 2']])\n\n# Count the occurrences of each variable\nvariable_counts = all_variables.value_counts()\n\n# Convert to a DataFrame for better visualization\nvariable_counts_df = variable_counts.reset_index()\nvariable_counts_df.columns = ['Variable', 'Count']\n\n# Display the result\nvariable_counts_df\n\n\n\n  \n    \n\n\n\n\n\n\nVariable\nCount\n\n\n\n\n0\nCereal production (metric tons)\n4\n\n\n1\nLabor force, total\n4\n\n\n2\nPopulation, total\n4\n\n\n3\nRural population\n4\n\n\n4\nArable land (hectares)\n4\n\n\n5\nNet foreign assets (current LCU)\n3\n\n\n6\nSubsidies and other transfers (current LCU)\n3\n\n\n7\nTaxes on goods and services (current LCU)\n3\n\n\n8\nNet domestic credit (current LCU)\n3\n\n\n9\nExternal debt stocks, total (DOD, current US$)\n2\n\n\n10\nDebt service on external debt, total (TDS, cur...\n1\n\n\n11\nSelf-employed, total (% of total employment) (...\n1\n\n\n12\nUrban population (% of total population)\n1\n\n\n13\nGDP (current US$)\n1\n\n\n14\nUrban population growth (annual %)\n1\n\n\n15\nEmployment in agriculture (% of total employme...\n1\n\n\n16\nPopulation growth (annual %)\n1\n\n\n17\nRural population (% of total population)\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nTo address multicollinearity issues in the dataset, specific variables have been selected for exclusion based on their strong correlations with others. These variables, listed in corrExclude, are removed to reduce redundancy and improve the reliability and interpretability of subsequent analyses. This decision ensures that the remaining variables provide unique and meaningful contributions to the models and analyses.\n\n\nCode\n# Decide to exclude some variables in response of collinearity issue\ncorrExclude = ['AG.PRD.CREL.MT',\n'DT.DOD.DECT.CD',\n'FM.AST.NFRG.CN',\n'GC.TAX.GSRV.CN',\n'SL.EMP.SELF.ZS',\n'BX.GSR.GNFS.CD',\n'DT.DOD.DECT.CD',\n'FM.AST.DOMS.CN',\n'FM.AST.NFRG.CN',\n'GC.TAX.GSRV.CN',\n'SP.RUR.TOTL.ZS',\n]\n\n\nEnriches the dataset by adding additional categorical information. The filtered allCountry DataFrame, containing region and incomeLevel data, is merged with the cleaned data (preProceedData2) based on country names.\n\n\nCode\n#Additional categorical data\n#Select only the required columns\nallCountryFiltered = allCountry[['name', 'region', 'incomeLevel']]\n\nadditionalMerged = pd.merge(preProceedData2, allCountryFiltered, left_on=['Country'], right_on=['name'], how='inner')\n\n\nIdentify variables to exclude from the dataset by combining two exclusion lists: corrExclude, which contains variables removed due to multicollinearity, and specific_excludes, which includes columns not relevant for analysis (e.g., identifiers like ID and name).\n\n\nCode\n# Columns to exclude from preProceedData2\nspecific_excludes = ['index', 'ID', 'AG.LND.FRST.K2','name']\n\n# Combine corrExclude with specific columns to exclude\nvariableXExclude = list(set(corrExclude + specific_excludes))\n\n\nCreates a cleaned dataset, ensuring that redundant, irrelevant, or highly correlated variables are excluded, leaving a refined dataset ready for further analysis or modeling.\n\n\nCode\ndataForRf = additionalMerged[[col for col in additionalMerged.columns if col not in variableXExclude]]\ndataForRf.round(1)\n\n\n\n  \n    \n\n\n\n\n\n\nCountry\nyear\nAG.LND.AGRI.K2\nAG.LND.FRST.ZS\nAG.LND.AGRI.ZS\nAG.LND.ARBL.HA\nAG.LND.CROP.ZS\nAG.PRD.CROP.XD\nEN.POP.DNST\nGE.EST\nNV.AGR.TOTL.ZS\nNY.ADJ.DFOR.CD\nNY.GDP.FRST.RT.ZS\nNY.GDP.MKTP.KD.ZG\nPV.EST\nRQ.EST\nSE.PRM.ENRR\nSE.SEC.ENRR\nSL.AGR.EMPL.ZS\nSL.TLF.TOTL.IN\nSN.ITK.DEFC.ZS\nSP.POP.GROW\nSP.POP.TOTL\nSP.RUR.TOTL\nSP.RUR.TOTL.ZG\nSP.URB.GROW\nSP.URB.TOTL.IN.ZS\nTM.VAL.AGRI.ZS.UN\nTM.VAL.MRCH.XD.WD\nTX.VAL.AGRI.ZS.UN\nBG.GSR.NFSV.GD.ZS\nBM.GSR.FCTY.CD\nBM.GSR.TRVL.ZS\nCC.EST\nDT.DOD.DIMF.CD\nDT.TDS.DECT.CD\nDT.TDS.MLAT.CD\nFP.CPI.TOTL\nGC.XPN.TRFT.CN\nNY.GDP.MKTP.CD\nDeforestation\nregion\nincomeLevel\n\n\n\n\n0\nAlbania\n2003\n11210.0\n28.2\n40.9\n578000.0\n4.4\n54.9\n110.9\n-0.6\n22.0\n3535170.7\n0.1\n5.5\n-0.3\n-0.5\n107.6\n77.8\n48.0\n1290479.0\n7.4\n-0.4\n3039616.0\n1684768.0\n-2.3\n2.1\n44.6\n1.0\n43.3\n5.1\n27.6\n2.436447e+07\n60.9\n-0.9\n1.594152e+08\n5.727977e+07\n2.021326e+07\n82.8\n5.728388e+10\n5.611496e+09\n12.8\nECS\nUMC\n\n\n1\nAlbania\n2004\n11220.0\n28.3\n40.9\n578000.0\n4.4\n58.5\n110.5\n-0.4\n20.5\n3879499.9\n0.1\n5.5\n-0.4\n-0.2\n104.1\n79.0\n47.6\n1275704.0\n8.6\n-0.4\n3026939.0\n1645111.0\n-2.4\n2.0\n45.7\n1.1\n53.7\n4.5\n30.0\n2.832284e+07\n60.8\n-0.7\n1.690999e+08\n7.602167e+07\n2.783090e+07\n84.7\n7.013519e+10\n7.184686e+09\n12.8\nECS\nUMC\n\n\n2\nAlbania\n2011\n12010.0\n28.6\n43.8\n622000.0\n2.7\n87.4\n106.0\n-0.2\n18.2\n27102197.8\n0.2\n2.5\n-0.3\n0.3\n101.6\n95.5\n45.4\n1376967.0\n4.8\n-0.3\n2905195.0\n1358266.0\n-2.6\n1.8\n53.2\n1.0\n125.4\n2.6\n39.3\n2.986859e+08\n69.6\n-0.7\n1.172413e+08\n4.825108e+08\n8.207203e+07\n103.4\n1.358977e+11\n1.289076e+10\n90.9\nECS\nUMC\n\n\n3\nAlbania\n2012\n12013.0\n28.6\n43.8\n619100.0\n2.8\n93.9\n105.9\n-0.3\n18.8\n22486375.5\n0.2\n1.4\n-0.1\n0.2\n102.1\n97.6\n46.0\n1317523.0\n4.5\n-0.2\n2900401.0\n1324613.0\n-2.5\n1.8\n54.3\n0.9\n113.5\n2.5\n34.9\n3.329938e+08\n68.6\n-0.8\n1.070361e+08\n5.425489e+08\n8.957130e+07\n105.5\n1.399201e+11\n1.231983e+10\n14.2\nECS\nUMC\n\n\n4\nAlbania\n2013\n11873.0\n28.7\n43.3\n617100.0\n2.9\n93.7\n105.7\n-0.3\n19.6\n21100579.4\n0.2\n1.0\n0.1\n0.3\n101.5\n102.9\n44.2\n1217209.0\n4.4\n-0.2\n2895092.0\n1291587.0\n-2.5\n1.7\n55.4\n1.0\n113.9\n2.3\n33.4\n1.504106e+08\n74.7\n-0.8\n9.675547e+07\n5.504619e+08\n1.013739e+08\n107.6\n1.517697e+11\n1.277622e+10\n14.2\nECS\nUMC\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n604\nUkraine\n2017\n414890.0\n16.7\n71.6\n32773000.0\n1.5\n100.8\n78.4\n-0.5\n10.2\n0.0\n0.4\n2.4\n-1.9\n-0.3\n88.2\n86.8\n15.4\n21646826.0\n2.6\n-0.4\n45436041.0\n13973400.0\n-0.7\n-0.3\n69.2\n1.2\n132.2\n1.7\n24.6\n7.691000e+09\n53.4\n-0.8\n1.400143e+10\n1.322454e+10\n2.139854e+09\n235.3\n6.123422e+11\n1.120905e+11\n70.0\nECS\nUMC\n\n\n605\nUkraine\n2018\n413290.0\n16.7\n71.3\n32889000.0\n1.5\n111.1\n78.0\n-0.5\n10.1\n0.0\n0.4\n3.5\n-1.9\n-0.3\n88.6\n85.8\n15.3\n21532913.0\n3.0\n-0.5\n45208907.0\n13855626.0\n-0.8\n-0.3\n69.4\n1.1\n152.4\n2.0\n23.2\n1.058100e+10\n54.5\n-0.9\n1.299723e+10\n1.476636e+10\n2.223670e+09\n261.1\n7.081612e+11\n1.308911e+11\n70.0\nECS\nUMC\n\n\n606\nUkraine\n2019\n413110.0\n16.7\n71.3\n32924000.0\n1.5\n112.8\n77.6\n-0.3\n9.0\n0.0\n0.3\n3.2\n-1.4\n-0.2\n90.7\n84.5\n15.2\n21427949.0\n3.7\n-0.6\n44957458.0\n13724163.0\n-1.0\n-0.4\n69.5\n1.0\n162.1\n1.7\n21.6\n1.137100e+10\n54.2\n-0.8\n1.132839e+10\n1.395323e+10\n1.250666e+09\n281.7\n7.612929e+11\n1.538830e+11\n60.0\nECS\nUMC\n\n\n607\nUkraine\n2020\n413110.0\n16.7\n71.3\n32924000.0\n1.5\n100.9\n77.1\n-0.4\n9.3\n0.0\n0.3\n-3.8\n-1.2\n-0.3\n91.1\n84.4\n15.3\n20852330.0\n4.3\n-0.6\n44680014.0\n13579150.0\n-1.1\n-0.4\n69.6\n1.1\n144.8\n1.7\n17.1\n8.629000e+09\n42.0\n-0.8\n1.282442e+10\n1.739990e+10\n1.304253e+09\n289.4\n8.467340e+11\n1.566177e+11\n60.0\nECS\nUMC\n\n\n608\nUkraine\n2021\n413110.0\n16.7\n71.3\n32924000.0\n1.5\n121.7\n76.5\n-0.4\n10.9\n0.0\n0.2\n3.4\n-1.1\n-0.3\n91.1\n84.7\n15.1\n20539091.0\n5.3\n-0.9\n44298640.0\n13397238.0\n-1.3\n-0.6\n69.8\n1.0\n193.3\n1.6\n16.4\n1.982900e+10\n43.3\n-0.8\n1.450520e+10\n1.449359e+10\n6.751136e+08\n316.4\n9.296944e+11\n1.997659e+11\n60.0\nECS\nUMC\n\n\n\n\n609 rows × 43 columns"
  },
  {
    "objectID": "analysis/Final_project.html#data-preparation-for-modeling",
    "href": "analysis/Final_project.html#data-preparation-for-modeling",
    "title": "Analysis",
    "section": "Data preparation for modeling",
    "text": "Data preparation for modeling\nSplit the refined dataset dataForRf into two subsets: 70% for training (train_set) and 30% for testing (test_set). The random_state=42 ensures reproducibility, allowing the same split to be generated each time.\n\n\nCode\n# Split the data 70/30\ntrain_set, test_set = train_test_split(dataForRf, test_size=0.3, random_state=42)\n\n\nDefine the target variable for the model, which is “Deforestation.” The Deforestation column is extracted separately from the training (y_train) and testing (y_test) datasets.\n\n\nCode\n# the target labels: log of Deforestation\ny_train = train_set[\"Deforestation\"]\ny_test = test_set[\"Deforestation\"]\n\n\nPrepare the features for modeling by separating them into numerical and categorical variables. First, the target variable Deforestation is dropped from the dataset (dropDefo) to focus only on predictors. The remaining columns are divided based on their data type: num_cols contains numerical features, and cat_cols contains categorical features.\n\n\nCode\ndropDefo = dataForRf.drop(columns=[\"Deforestation\"])\n\n# Divide data by type\nnum_cols = sorted(dropDefo.select_dtypes(include=['int64', 'float64']).columns.tolist())\ncat_cols = sorted(dropDefo.select_dtypes(include=['object']).columns.tolist())\n\nprint(f'There are {len(cat_cols)} categorical variables')\nprint(f'There are {len(num_cols)} numerical variables')\n\n\nThere are 3 categorical variables\nThere are 39 numerical variables\n\n\nSet up a preprocessing pipeline using a ColumnTransformer to handle both numerical and categorical features. Numerical columns (num_cols) are scaled with StandardScaler to normalize their values, while categorical columns (cat_cols) are encoded into binary format using OneHotEncoder, which creates a new binary column for each unique category. The processed feature sets, X_train and X_test, are extracted from the training and testing datasets.\n\n\nCode\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\nX_train = train_set[num_cols + cat_cols]\nX_test = test_set[num_cols + cat_cols]"
  },
  {
    "objectID": "analysis/Final_project.html#pipeline",
    "href": "analysis/Final_project.html#pipeline",
    "title": "Analysis",
    "section": "Pipeline",
    "text": "Pipeline\nInitialize a machine learning pipeline combining data preprocessing and modeling. The transformer handles feature preprocessing (scaling numerical features and one-hot encoding categorical features), while the RandomForestRegressor is the chosen model for regression tasks.\n\n\nCode\n# Initialize the pipeline\npipe = make_pipeline(\n    transformer, RandomForestRegressor(n_estimators=10,\n                                       random_state=42)\n)"
  },
  {
    "objectID": "analysis/Final_project.html#base-random-forest-model",
    "href": "analysis/Final_project.html#base-random-forest-model",
    "title": "Analysis",
    "section": "Base Random Forest Model",
    "text": "Base Random Forest Model\nTrain the machine learning pipeline by fitting it to the training data (train_set) and the target labels (y_train). The pipeline automatically applies preprocessing (scaling and encoding) to the features and then trains the RandomForestRegressor model on the processed data.\n\n\nCode\n# Fit the training set\npipe.fit(train_set, y_train);\n\n\nEvaluate the performance of the trained pipeline on the testing dataset. The pipe.score method computes the R² (coefficient of determination) score, which measures how well the model’s predictions match the actual target values (y_test). An R² score close to 1 indicates a strong predictive performance, while lower values suggest areas for improvement.\n\n\nCode\n# What's the test score?\npipe.score(test_set, y_test)\n\n\n0.6748279503793474\n\n\n\n\nCode\ntestscore1 = pipe.score(test_set, y_test)"
  },
  {
    "objectID": "analysis/Final_project.html#feature-importance",
    "href": "analysis/Final_project.html#feature-importance",
    "title": "Analysis",
    "section": "Feature importance",
    "text": "Feature importance\nFeature importance is computed to identify the most significant variables contributing to deforestation.\nRetrieve the names of the transformed features after one-hot encoding. The one-hot encoder (ohe) within the pipeline is accessed to obtain the generated binary columns (ohe_cols) for categorical variables. These are combined with the original numerical column names (num_cols) to create a comprehensive list of feature names (features).\n\n\nCode\n# The one-hot step\nohe = transformer.named_transformers_['cat']\n\n# One column for each category type!\nohe_cols = ohe.get_feature_names_out()\n\n# Full list of columns is numerical + one-hot\nfeatures = num_cols + list(ohe_cols)\n\n\nCalculate the importance of each feature in the RandomForestRegressor. The trained random forest model is accessed from the pipeline (random_forest), and its feature_importances_ attribute provides the relative contribution of each feature to the model’s predictions. These importance values are combined with the feature names (features) into a DataFrame (importance).\n\n\nCode\nrandom_forest = pipe[\"randomforestregressor\"]\n\n# Create the dataframe with importances\nimportance = pd.DataFrame(\n    {\"Feature\": features, \"Importance\": random_forest.feature_importances_}\n)\n\n\n\nVisualization of feature importance\nThe importance DataFrame is merged with codeDescription to map each feature to its corresponding description. If a description is unavailable, the original feature name is retained. The resulting DataFrame, importanceResult, contains only the feature descriptions and their importance scores.\n\n\nCode\n# Change code to description\nimportanceResult = pd.merge(\n    importance,\n    codeDescription,\n    left_on='Feature',\n    right_on='Variable',\n    how='left'\n)\n\nimportanceResult = importanceResult.drop(columns=['Variable'])\nimportanceResult['Description'] = importanceResult['Description'].fillna(importanceResult['Feature'])\nimportanceResult = importanceResult.drop(columns=['Feature'])\n\n\nVisualize the top 20 most important features in the Random Forest model using a bar chart. The importanceResult DataFrame is sorted by the Importance column in descending order, and the top 20 features are selected. Altair is used to create a bar chart where the x-axis represents feature importance, and the y-axis lists feature descriptions sorted by their importance. Tooltips provide additional detail, and the chart is styled with a clear title and appropriate dimensions.\n\n\nCode\n# Sort the data and select the top 50 features\ntop_features = importanceResult.sort_values(\"Importance\", ascending=False).round(5).head(20)\n\n# Create the Feature Importance plot using Altair\nfeature_importance_chart = alt.Chart(top_features).mark_bar().encode(\n    x=alt.X(\"Importance:Q\", title=\"Importance\"),\n    y=alt.Y(\"Description:N\", sort='-x', title=\"Descriptions\"),\n    tooltip=[\"Description:N\", \"Importance:Q\"]\n).properties(\n    title=\"Top 20 Feature Importance Plot\",\n    width=600,\n    height=400\n).configure_title(\n    fontSize=16,\n    anchor='middle'\n)\n\nfeature_importance_chart.show()\n\n\n\n\n\n\n\n\nEvaluate the pipeline’s performance using 3-fold cross-validation. The dataset is split into three subsets, and the model is trained and tested on different combinations of these subsets to calculate R² scores (scores). These scores measure how well the model explains the variance in the target variable. The mean (scores.mean()) and standard deviation (scores.std()) of the scores are calculated to summarize the model’s overall performance and consistency across the folds.\n\n\nCode\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    pipe,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Copy\nscores1 = scores\nscoresmean1 = scores.mean\nscoresstd1 = scores.std\n\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.85363655 0.71871286 0.55555343]\nScores mean =  0.7093009488161712\nScore std dev =  0.12187377264188007"
  },
  {
    "objectID": "analysis/Final_project.html#gridsearchcv",
    "href": "analysis/Final_project.html#gridsearchcv",
    "title": "Analysis",
    "section": "GridSearchCV",
    "text": "GridSearchCV\nThe grid search systematically tests all combinations of the hyperparameters defined in param_grid by training the model with 3-fold cross-validation (cv=3) for each combination. The grid.fit method trains the pipeline (pipeGridSearch) on the training data (train_set, y_train) to identify the best-performing hyperparameter settings. The verbose=1 option provides progress updates during the search process.\n\n\nCode\n# Create the grid and use 3-fold CV\ngrid = GridSearchCV(pipeGridSearch, param_grid, cv=3, verbose=1)\n\n# Run the search\ngrid.fit(train_set, y_train)\n\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['AG.LND.AGRI.K2',\n                                                                          'AG.LND.AGRI.ZS',\n                                                                          'AG.LND.ARBL.HA',\n                                                                          'AG.LND.CROP.ZS',\n                                                                          'AG.LND.FRST.ZS',\n                                                                          'AG.PRD.CROP.XD',\n                                                                          'BG.GSR.NFSV.GD.ZS',\n                                                                          'BM.GSR.FCTY.CD',\n                                                                          'BM.GSR.TRVL.ZS',\n                                                                          'CC.EST',\n                                                                          'DT.DOD.DIMF.CD',\n                                                                          'DT.TDS.DECT.CD',\n                                                                          'DT.TDS.MLAT.CD',\n                                                                          'EN.POP.DNST...\n                                                                          'SL.TLF.TOTL.IN',\n                                                                          'SN.ITK.DEFC.ZS',\n                                                                          'SP.POP.GROW', ...]),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['Country',\n                                                                          'incomeLevel',\n                                                                          'region'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['AG.LND.AGRI.K2',\n                                                                          'AG.LND.AGRI.ZS',\n                                                                          'AG.LND.ARBL.HA',\n                                                                          'AG.LND.CROP.ZS',\n                                                                          'AG.LND.FRST.ZS',\n                                                                          'AG.PRD.CROP.XD',\n                                                                          'BG.GSR.NFSV.GD.ZS',\n                                                                          'BM.GSR.FCTY.CD',\n                                                                          'BM.GSR.TRVL.ZS',\n                                                                          'CC.EST',\n                                                                          'DT.DOD.DIMF.CD',\n                                                                          'DT.TDS.DECT.CD',\n                                                                          'DT.TDS.MLAT.CD',\n                                                                          'EN.POP.DNST...\n                                                                          'SL.TLF.TOTL.IN',\n                                                                          'SN.ITK.DEFC.ZS',\n                                                                          'SP.POP.GROW', ...]),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['Country',\n                                                                          'incomeLevel',\n                                                                          'region'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1) best_estimator_: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['AG.LND.AGRI.K2',\n                                                   'AG.LND.AGRI.ZS',\n                                                   'AG.LND.ARBL.HA',\n                                                   'AG.LND.CROP.ZS',\n                                                   'AG.LND.FRST.ZS',\n                                                   'AG.PRD.CROP.XD',\n                                                   'BG.GSR.NFSV.GD.ZS',\n                                                   'BM.GSR.FCTY.CD',\n                                                   'BM.GSR.TRVL.ZS', 'CC.EST',\n                                                   'DT.DOD.DIMF.CD',\n                                                   'DT.TDS.DECT.CD',\n                                                   'DT.TDS.MLAT.CD',\n                                                   'EN.POP.DNST', 'FP.CPI.TOTL',\n                                                   'GC.XPN.TRFT...\n                                                   'NY.GDP.FRST.RT.ZS',\n                                                   'NY.GDP.MKTP.CD',\n                                                   'NY.GDP.MKTP.KD.ZG',\n                                                   'PV.EST', 'RQ.EST',\n                                                   'SE.PRM.ENRR', 'SE.SEC.ENRR',\n                                                   'SL.AGR.EMPL.ZS',\n                                                   'SL.TLF.TOTL.IN',\n                                                   'SN.ITK.DEFC.ZS',\n                                                   'SP.POP.GROW', ...]),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Country', 'incomeLevel',\n                                                   'region'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=13, n_estimators=50,\n                                       random_state=42))]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['AG.LND.AGRI.K2', 'AG.LND.AGRI.ZS',\n                                  'AG.LND.ARBL.HA', 'AG.LND.CROP.ZS',\n                                  'AG.LND.FRST.ZS', 'AG.PRD.CROP.XD',\n                                  'BG.GSR.NFSV.GD.ZS', 'BM.GSR.FCTY.CD',\n                                  'BM.GSR.TRVL.ZS', 'CC.EST', 'DT.DOD.DIMF.CD',\n                                  'DT.TDS.DECT.CD', 'DT.TDS.MLAT.CD',\n                                  'EN.POP.DNST', 'FP.CPI.TOTL',\n                                  'GC.XPN.TRFT.CN', 'GE.EST', 'NV.AGR.TOTL.ZS',\n                                  'NY.ADJ.DFOR.CD', 'NY.GDP.FRST.RT.ZS',\n                                  'NY.GDP.MKTP.CD', 'NY.GDP.MKTP.KD.ZG',\n                                  'PV.EST', 'RQ.EST', 'SE.PRM.ENRR',\n                                  'SE.SEC.ENRR', 'SL.AGR.EMPL.ZS',\n                                  'SL.TLF.TOTL.IN', 'SN.ITK.DEFC.ZS',\n                                  'SP.POP.GROW', ...]),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['Country', 'incomeLevel', 'region'])]) num['AG.LND.AGRI.K2', 'AG.LND.AGRI.ZS', 'AG.LND.ARBL.HA', 'AG.LND.CROP.ZS', 'AG.LND.FRST.ZS', 'AG.PRD.CROP.XD', 'BG.GSR.NFSV.GD.ZS', 'BM.GSR.FCTY.CD', 'BM.GSR.TRVL.ZS', 'CC.EST', 'DT.DOD.DIMF.CD', 'DT.TDS.DECT.CD', 'DT.TDS.MLAT.CD', 'EN.POP.DNST', 'FP.CPI.TOTL', 'GC.XPN.TRFT.CN', 'GE.EST', 'NV.AGR.TOTL.ZS', 'NY.ADJ.DFOR.CD', 'NY.GDP.FRST.RT.ZS', 'NY.GDP.MKTP.CD', 'NY.GDP.MKTP.KD.ZG', 'PV.EST', 'RQ.EST', 'SE.PRM.ENRR', 'SE.SEC.ENRR', 'SL.AGR.EMPL.ZS', 'SL.TLF.TOTL.IN', 'SN.ITK.DEFC.ZS', 'SP.POP.GROW', 'SP.POP.TOTL', 'SP.RUR.TOTL', 'SP.RUR.TOTL.ZG', 'SP.URB.GROW', 'SP.URB.TOTL.IN.ZS', 'TM.VAL.AGRI.ZS.UN', 'TM.VAL.MRCH.XD.WD', 'TX.VAL.AGRI.ZS.UN', 'year'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['Country', 'incomeLevel', 'region'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(max_depth=13, n_estimators=50, random_state=42) \n\n\ngrid.best_estimator_: Displays the pipeline with the best combination of hyperparameters, which achieved the highest performance during cross-validation. grid.best_params_: Shows the specific hyperparameter values (e.g., n_estimators and max_depth) that resulted in the best model performance.\n\n\nCode\n# The best estimator\ngrid.best_estimator_\n\n# The best hyper parameters\ngrid.best_params_\n\n\n{'randomforestregressor__max_depth': 13,\n 'randomforestregressor__n_estimators': 50}\n\n\nThe model (best_model) predicts the target values (y_pred) for the test set (X_test), and its performance is measured using the R² score, which assesses how well the model explains the variance in the actual target values (y_test). The results include the average cross-validation score during training (grid.best_score_) and the R² score on the testing data.\n\n\nCode\n# Compute recall score on the testing set using the best model\nbest_model = grid.best_estimator_\ny_pred = best_model.predict(X_test)\nsearch_score = r2_score(y_test, y_pred)\ntestscore2 = search_score\n\nprint(f'The best model achieves an average cross-validation score of {grid.best_score_*100:.2f}%')\nprint(f'The best model achieves a r2 score of {search_score*100:.2f}% on the testing data')\n\n\nThe best model achieves an average cross-validation score of 73.78%\nThe best model achieves a r2 score of 67.43% on the testing data\n\n\n3-fold cross-validation to evaluate the performance of the pipeline (pipeGridSearch) on the training data (X_train and y_train). It splits the data into three subsets, trains the model on two subsets, and tests it on the third, repeating this process for all folds. The R² scores from each fold are recorded, and their mean and standard deviation are calculated to summarize the model’s overall performance and consistency.\n\n\nCode\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    pipeGridSearch,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Copy\nscores2 = scores\nscoresmean2 = scores.mean\nscoresstd2 = scores.std\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.86553356 0.74053747 0.56055631]\nScores mean =  0.7222091138420561\nScore std dev =  0.1251791455131474\n\n\n\nVisualization of accuracy comparison\nCompare the performance of the initial and optimized models by summarizing their R² scores from cross-validation and testing. The scores for each fold, along with the mean and test scores, are recorded for both models. The data is structured into a DataFrame (slope_df) to prepare for visualization, highlighting the performance improvements achieved through hyperparameter optimization using GridSearchCV.\n\n\nCode\n# Comparing before and after optimization using GridSearchCV\n# Data for initial model\ninitial_cv_scores = scores1\ninitial_mean = scoresmean1()\ninitial_std = scoresstd1()\ninitial_test_score = testscore1\n\n# Data for optimized model\noptimized_cv_scores = scores2\noptimized_mean = scoresmean2()\noptimized_std = scoresstd2()\noptimized_test_score = testscore2\n\n# Data for the slope graph\nmetrics = ['Fold 1', 'Fold 2', 'Fold 3', 'Mean', 'Test Score']\ninitial_scores = initial_cv_scores.tolist() + [initial_mean, initial_test_score]\noptimized_scores = optimized_cv_scores.tolist() + [optimized_mean, optimized_test_score]\n\n# Prepare the data for the slope graph\nslope_data = {\n    'Metric': metrics * 2,  # Repeat metrics for 'Initial' and 'Optimized'\n    'Score': initial_scores + optimized_scores,\n    'Model': ['Initial'] * len(metrics) + ['Optimized'] * len(metrics)\n}\n\n# Convert to DataFrame\nslope_df = pd.DataFrame(slope_data)\n\n\nVisualize the comparison between the initial and optimized models using a slope graph created with Altair. The chart displays R² scores for each metric (e.g., folds, mean, and test score) as lines connecting the initial and optimized models, making it easy to observe performance improvements. Custom colors are used to differentiate metrics, and tooltips provide detailed information on scores and models.\n\n\nCode\n# Create chart\n# Define custom colors for each metric\ncustom_colors = {\n    'Fold 1': '#82C574',\n    'Fold 2': '#EB9E42',\n    'Fold 3': '#DF7068',\n    'Mean': '#7DC1DD',\n    'Test Score': '#3F597C'\n}\n\n# Create the slope graph\nslope_chart = alt.Chart(slope_df).mark_line(point=alt.MarkConfig(size=100)).encode(\n    x=alt.X('Model:N', title='', axis=alt.Axis(labels=True, labelAngle=0)),\n    y=alt.Y('Score:Q', title='R² Score', scale=alt.Scale(domain=[0.5, 1])),\n    color=alt.Color('Metric:N', scale=alt.Scale(domain=list(custom_colors.keys()), range=list(custom_colors.values()))),\n    detail='Metric:N',\n    tooltip=['Metric:N', 'Score:Q', 'Model:N']\n).properties(\n    title=\"R² Score: Initial vs Optimized (GridSearchCV) Model\",\n    width=400,\n    height=300\n).configure_title(\n    fontSize=16,\n    anchor='middle'\n).configure_axis(\n    labelFontSize=12,\n    titleFontSize=14\n)\n\nslope_chart.show()"
  },
  {
    "objectID": "analysis/Final_project.html#comparison-of-tested-vs.-predicted-deforestation",
    "href": "analysis/Final_project.html#comparison-of-tested-vs.-predicted-deforestation",
    "title": "Analysis",
    "section": "Comparison of Tested vs. Predicted Deforestation",
    "text": "Comparison of Tested vs. Predicted Deforestation\n\nVisualization of y test and y prediction\nAltair visualizations are created to compare actual deforestation trends (y_test) with model predictions (y_pred). A shared brush selection allows users to focus on specific countries.\nA new DataFrame, predVis, is created to store predictions along with associated metadata, such as year, Country, and region. The region_full column maps region abbreviations to their full names for clarity. Similarly, the actual deforestation values are combined with their corresponding metadata in mergeXYTest.\n\n\nCode\n# Visualization: Comparing between actual vs prediction\npredVis = pd.DataFrame({'Predictions': y_pred})\npredVis['year'] = X_test['year'].reset_index(drop=True)\npredVis['Country'] = X_test['Country'].reset_index(drop=True)\npredVis['region'] = X_test['region'].reset_index(drop=True)\npredVis['region_full'] = predVis['region'].map(region_mapping)\n\ndfYTest = pd.DataFrame(y_test)\nX_test_index = X_test.reset_index()\ndfYTest = dfYTest.reset_index()\nmergeXYTest = pd.merge(dfYTest, X_test_index, on='index')\nmergeXYTest = mergeXYTest[['Deforestation', 'Country', 'year', 'region']]\nmergeXYTest['region_full'] = mergeXYTest['region'].map(region_mapping)\n\n\nCreate a side-by-side comparison of observed (y_test) and predicted (y_pred) deforestation trends using Altair. Two bar charts are generated: one for reported deforestation (mergeXYTest) and another for predicted values (predVis). Both charts are grouped by year and colored by regions, with custom colors and tooltips providing additional details. A shared brush selection allows users to click on a specific country to highlight its data in both charts simultaneously. The charts are combined horizontally to visualize actual versus predicted trends side-by-side.\n\n\nCode\n# Define custom colors\ncustomColor = ['#7DC1DD', '#EB9E42', '#DF7068', '#82C574', '#82888D', '#3F597C']\n\n# Shared brush selection for both charts\nshared_brush = alt.selection_point(fields=['Country'], on='click', name='shared')\n\n# Plot 1: Reported Deforestation\ndeforestation_chart = alt.Chart(mergeXYTest).mark_bar(stroke='white', strokeWidth=0.5).encode(\n    x=alt.X('year:O', title='Year'),\n    y=alt.Y('sum(Deforestation):Q', title='Change in Forest Area, sq. km (Negative = forest loss)'),\n    color=alt.condition(\n        shared_brush,\n        alt.Color(\"region_full:N\", scale=alt.Scale(range=customColor), legend=alt.Legend(title=\"Region\", orient=\"bottom-left\")),\n        alt.value('lightgray')\n    ),\n    tooltip=['Country', 'year', 'Deforestation', 'region_full']\n).add_params(\n    shared_brush\n).properties(\n    title='Observed Deforestation (y_test)',\n    width=400,\n    height=300\n)\n\n# Plot 2: Predicted Deforestation\npredDefoChart = alt.Chart(predVis).mark_bar(stroke='white', strokeWidth=0.5).encode(\n    x=alt.X('year:O', title='Year'),\n    y=alt.Y('sum(Predictions):Q', title='Change in Forest Area, sq. km (Negative = forest loss)'),\n    color=alt.condition(\n        shared_brush,\n        alt.Color(\"region_full:N\", scale=alt.Scale(range=customColor), legend=alt.Legend(title=\"Region\", orient=\"bottom-left\")),\n        alt.value('lightgray')\n    ),\n    tooltip=['Country', 'year', 'Predictions', 'region_full']\n).add_params(\n    shared_brush\n).properties(\n    title='Predicted Deforestation (y_pred)',\n    width=400,\n    height=300\n)\n\n# Combine charts horizontally\ncombined_chart = alt.hconcat(\n    deforestation_chart,\n    predDefoChart\n).properties(\n    title=\"Comparison\"\n).configure_legend(\n    orient='bottom-left',\n    titleFontSize=12,\n    labelFontSize=10,\n    fillColor='white'\n).configure_title(\n    fontSize=16,\n    anchor='middle',\n    color='black'\n)\n\n# Show the combined chart\ncombined_chart"
  },
  {
    "objectID": "analysis/Final_project.html#residual-analysis",
    "href": "analysis/Final_project.html#residual-analysis",
    "title": "Analysis",
    "section": "Residual analysis",
    "text": "Residual analysis\nResiduals (differences between actual and predicted values) are analyzed to check for systematic biases in the model.\nThe residuals are stored in a new DataFrame, yResidual, providing insight into the errors made by the model. A copy of this DataFrame, yResidualVis, is created for visualization, and it includes the predicted values alongside the residuals to help analyze patterns and identify systematic biases in the model’s predictions.\n\n\nCode\n# Calculate residuals\nyResidual = pd.DataFrame()\nyResidual['Residuals'] = mergeXYTest['Deforestation'] - predVis['Predictions']\n\n# Create dataframe for visualization\nyResidualVis = yResidual.copy()\nyResidualVis['Predictions'] = predVis['Predictions']\n\n\n\nVisualization of residual\nCreate a residual plot to evaluate the performance of the regression model. Using Seaborn’s scatterplot, it plots predicted values (Predictions) on the x-axis and residuals (Residuals) on the y-axis. A horizontal dashed line at y=0 is added as a reference, showing where residuals would indicate perfect predictions. The plot is styled with titles, labels, and a grid for clarity. Randomly scattered residuals around the zero line suggest the model performs well.\n\n\nCode\n# Create a scatter plot for residuals\nfig, axes = plt.subplots(1, 1, figsize=(6, 4))\n\n# Create a scatter plot of residuals\nsns.scatterplot(\n    data=yResidualVis,\n    x='Predictions',  # X-axis as index of residuals\n    y='Residuals',  # Residuals on the Y-axis\n    color='#DF7068',\n    ax=axes\n)\naxes.axhline(0, color='#3F597C', linestyle='--', linewidth=1)  # Add a horizontal line at 0\naxes.set_title('Residual Plot', fontsize=11)\naxes.set_xlabel('Predictions', fontsize=11)\naxes.set_ylabel('Residuals', fontsize=11)\naxes.grid(True)\n\n# Show the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "findings.html",
    "href": "findings.html",
    "title": "Findings",
    "section": "",
    "text": "Your browser does not support the video tag. \n\n\n\n\n\n©Video by Desra\n\n\n\n\n\nMain findings focused on Feature Importance.\n\nBrazil, subsidies and other transfers, and agricultural land were the most significant contributors to deforestation among all variables.\n\n\n\n\n\n\nIn the Random Forest analysis, the feature “Country_Brazil” has high importance, not simply because Brazil has a large forest area, but because its deforestation trends are highly variable and strongly influenced by other factors like agricultural subsidies, political policies, and enforcement of conservation laws. For instance, Brazil experienced a deforestation peak in 2004 when 27,772 km² of forests were cleared (equivalent to the size of Massachusetts), followed by a decline during years with stricter policies (Silva Junior et al. 2021). This variability in deforestation rates makes Brazil-specific data critical in understanding global deforestation trends. Similarly, countries like Paraguay and Tanzania also rank highly due to their distinct patterns, such as agricultural expansion or population pressures, which uniquely influence their forest loss dynamics.\n\nBy defintion, Subsidies and Other Transfers benefits include all unrequited, nonrepayable transfers on current account to private and public enterprises; grants to foreign governments, international organizations, and other government units; and social security, social assistance benefits, and employer social benefits in cash and in kind. According to Amaglobeli et al. (2024), Agricultural producer subsidies are prevalent, large, and deployed to achieve diverse and, at times, overlapping policy objectives. Among countries accounting for 90 percent of global GDP, food and agriculture subsidies amount to 0.3–0.7 percent of GDP over the past decade and a half. As highlighted by Damania et al. (2023), highlighted inefficiency of agricultural and timber subsidies. Between 2016 and 2018, $635 billion per year (equals approximately 0.9 percent of GDP and nearly one-fifth of agricultural value added for these countries) was given as support to agriculture in 84 countries. 71% of this support went directly to farmers or producers, mainly in ways that encouraged them to produce more or use certain inputs, which can influence their decisions on what and how much to produce. This type of support often encourage unsustainable practices, such as excessive use of chemical fertilizers and pesticides, which lead to greenhouse gas emissions, land degradation, and biodiversity loss. Similarly, timber subsidies contribute to overharvesting, illegal logging, and forest degradation. They distort markets by making sustainable forest management less competitive than subsidized, unsustainable logging.\n\nThe feature Agricultural Land (sq. km) emerges as one of the most significant contributors to deforestation in the feature importance analysis. This result is in line with finding from Hosonuma et al. (2012). Commercial agriculture is the most important driver of deforestation, followed by subsistence agriculture. Their study showed agriculture alone causes 73% of all deforestation. 40% of deforestation and most prominent in the early-transition phase. The other important land use is local/subsistence agriculture, which is related to 33% of deforestation.\n\nOther factors that stand out in the analysis are governance and economic indicators, which reveal intriguing relationships with deforestation. For instance, control of corruption shows a notable connection to deforestation, suggesting that governance quality significantly influences how natural resources are managed. In regions with poor governance and high corruption, illegal logging, weak enforcement of environmental regulations, and unsustainable land-use practices are more prevalent, exacerbating deforestation rates. Conversely, better governance often correlates with stronger protections and more sustainable practices.\n\nAnother compelling factor is debt service on external debt, which indicates the economic pressures faced by nations. When a country allocates a significant portion of its resources toward debt repayment, it may prioritize short-term economic gains, such as expanding agricultural exports or extracting natural resources, to generate revenue. This often results in deforestation as forests are cleared to make way for cash crops or logging operations. Together, these governance and economic factors underline the complex, interconnected pressures that drive deforestation, pointing to the need for policies that address systemic issues like corruption and economic vulnerability to protect forests effectively.\n\n\n\n\n\n\nContemplation\n\nOver time, the theory of deforestation has evolved, moving from simplistic explanations centered on population growth and agricultural expansion to more nuanced, multidimensional frameworks. Earlier models, like those discussed by Angelsen and Kaimowitz (1999), focused on economic drivers and agent-based decision-making, emphasizing variables such as land-use choices, macroeconomic pressures, and policy instruments. Recent research has expanded these perspectives by incorporating the roles of governance, global trade, climate change, and spatial dynamics. Studies now recognize deforestation as a complex phenomenon influenced by intertwined local and global factors, such as subsidies driving unsustainable agricultural practices, corruption weakening forest protections, and international markets shaping land-use patterns. This evolution reflects a shift toward systems-level thinking, which integrates biophysical, socioeconomic, and governance dimensions. Thus, while foundational theories remain relevant, contemporary approaches offer a deeper, more holistic understanding of deforestation, better equipping policymakers to address its underlying and immediate causes in an increasingly interconnected world.\n\n\n\n\nWhat we are doing to the forests of the world is but a mirror reflection of what we are doing to ourselves and to one another. ~ Mahatma Gandhi\n\n\n\n\n\nAdditional discussions\n\nPrediction\nAlthough my analysis included predicting deforestation using socioeconomic factors with a Random Forest model, I believe that relying solely on these factors for such predictions may lack a solid logical foundation. Socioeconomic variables, while insightful, typically act as indirect drivers or proxies rather than immediate causes of deforestation. This perspective aligns with the framework proposed by Angelsen and Kaimowitz (1999), my main reference, which emphasizes the importance of analyzing deforestation through a more comprehensive approach. Their framework suggests considering the magnitude and location of deforestation, the agents involved, the variables influencing their choices, the decision parameters of these agents, as well as macroeconomic variables and policy instruments. A logical approach, as outlined in their work, involves understanding deforestation at three interconnected levels: sources, immediate causes, and underlying causes. By focusing only on socioeconomic factors, key dimensions such as the direct actions of deforestation agents (e.g., farmers or loggers), the biophysical constraints, and the specific policy environment might be overlooked.\n\nOptimization\nThe initial Random Forest model achieved an R² score of 0.67 on the testing dataset, indicating that 67% of the variance in the target variable was explained by the model. Using 3-fold cross-validation, the R² scores varied between 0.85, 0.71, and 0.56, with an average score of 0.71 and a standard deviation of 0.12, highlighting moderate variability in model performance across different folds. After optimizing the model using GridSearchCV, the testing R² score remained unchanged at 0.67. However, the 3-fold cross-validation scores showed slight improvement, with individual scores of 0.86, 0.74, and 0.56. This resulted in an increased average cross-validation score of 0.72, while the standard deviation remained at 0.12.\n\n\n\n\n\nReferences\n\nAmaglobeli, D. (2024). Agricultural producer subsidies. IMF Notes, 2024(002), 1. https://doi.org/10.5089/9798400285950.068.\n\nAngelsen, A., & Kaimowitz, D. (1999). Rethinking the Causes of Deforestation: Lessons from Economic Models. The World Bank Research Observer, 14(1), 73–98. https://doi.org/10.1093/wbro/14.1.73.\n\nDamania, R., Balseca, E., De Fontaubert, C., Gill, J., Kim, K., Rentschler, J., Russ, J., & Zaveri, E. (2023). Detox development. https://doi.org/10.1596/978-1-4648-1916-2.\n\nHosonuma, N., Herold, M., De Sy, V., De Fries, R. S., Brockhaus, M., Verchot, L., Angelsen, A., & Romijn, E. (2012). An assessment of deforestation and forest degradation drivers in developing countries. Environmental Research Letters, 7(4), 044009. https://doi.org/10.1088/1748-9326/7/4/044009.\n\nSilva Junior, C. H. L., Pessôa, A. C. M., Carvalho, N. S., et al. (2021). The Brazilian Amazon deforestation rate in 2020 is the greatest of the decade. Nature Ecology & Evolution, 5(2), 144–145. https://doi.org/10.1038/s41559-020-01368-x."
  },
  {
    "objectID": "thoughts.html",
    "href": "thoughts.html",
    "title": "Thoughts",
    "section": "",
    "text": "©Photo by Desra\n\n In 1999, researchers from the World Bank published an article titled “Rethinking the causes of deforestation: lesson from economic models”. This article synthesized the causes of deforestation based on hundreds of economic models available at that time. They highlighted a logical approach to analyzing deforestation at three different levels: sources (small farmers, loggers, etc.), immediate causes (prices, wages, technology, accessibility, etc.), and underlying causes (population, economic growth). A clear distinction among the three levels was based on the scale, from micro to macro influences. The findings showed only weak support for the thesis that population growth and poverty are driving forces of deforestation. Economic growth often drives deforestation by influencing higher agricultural and timber prices, while lower timber prices tend to reduce logging and agricultural encroachment. The tenure thesis suggests that secure land tenure can influence deforestation by incentivizing forest clearing. The intensification thesis revealed a mixed effect, as agricultural advancements may either exacerbate or alleviate deforestation.\nInspired by this article, I set out to “re-rethink” several variables that influence deforestation. I used the World Bank’s data API, a Python library that provides access to a wide range of global development data. This allowed me to analyze updated World Bank datasets using machine learning and examine the impact of various variables (see the list below) on deforestation.\nThis study utilized the Random Forest method to analyze the impact of various key drivers on deforestation. The main findings focused on Feature Importance. The process was carried out in the following steps:\n\n\n\n\n\n\n A total of 65 variables were selected and classified into 10 topic groups, including:\nAgriculture and land use\nAgricultural land (sq. km)\nForest area (% of land area)\nAgricultural land (% of land area)\nArable land (hectares)\nPermanent cropland (% of land area)\nForest area (sq. km)\nUrban land area (sq. km)\nCereal production (metric tons)\nCrop production index (2014-2016 = 100)\nFertilizer consumption (% of fertilizer production)\n\nEconomy\nAgriculture, forestry, and fishing, value added (% of GDP)\nAdjusted savings: net forest depletion (current USD)\nForest rents (% of GDP)\nGDP growth (annual %)\nAgricultural raw materials imports (% of merchandise imports)\nImport value index (2015 = 100)\nAgricultural raw materials exports (% of merchandise exports)\nTrade in services (% of GDP)\nPrimary income payments (BoP, current USD)\nTravel services (% of service imports, BoP)\nExports of goods and services (BoP, current USD)\nMarket capitalization of listed domestic companies (current USD)\nExternal debt stocks, total (DOD, current USD)\nUse of IMF credit (DOD, current USD)\nDebt service on external debt, total (TDS, current USD)\nMultilateral debt service (TDS, current USD)\nBorrowers from commercial banks (per 1,000 adults)\nNet domestic credit (current LCU)\nNet foreign assets (current LCU)\nConsumer price index (2010 = 100)\nTaxes on exports (current LCU)\nTaxes on goods and services (current LCU)\nTaxes on international trade (current LCU)\nSubsidies and other transfers (current LCU)\nLead time to export, median case (days)\nGDP (current USD)\n\nEducation\nLiteracy rate, adult total (% of people ages 15 and above)\nSchool enrollment, primary (% gross)\nSchool enrollment, secondary (% gross)\n\nEmployment\nChild employment in agriculture (% of economically active children ages 7-14)\nEmployment in agriculture (% of total employment) (modeled ILO estimate)\nSelf-employed, total (% of total employment) (modeled ILO estimate)\nLabor force, total\n\nEnergy\nFossil fuel energy consumption (% of total)\nEnergy use (kg of oil equivalent per capita)\n\nHealth\nPrevalence of underweight, weight for age (% of children under 5)\nPrevalence of undernourishment (% of population)\n\nInequality\nPoverty headcount ratio at D2.15 a day (2017 PPP) (% of population)\nGini index\n\nInstitutional quality\nGovernment Effectiveness: Estimate\nRegulatory Quality: Estimate\nControl of Corruption: Estimate\n\nPopulation\nPopulation density (people per sq. km of land area)\nPopulation growth (annual %)\nPopulation, total\nRural population\nRural population growth (annual %)\nRural population (% of total population)\nUrban population growth (annual %)\nUrban population (% of total population)\nRural land area (sq. km)\nHuman capital index (HCI) (scale 0-1)\nRail lines (total route-km)\n\nStability\nPolitical Stability and Absence of Violence/Terrorism: Estimate\nInternally displaced persons, total displaced by conflict and violence (number of people)\n\n\nReference\nAngelsen, A., & Kaimowitz, D. (1999). Rethinking the Causes of Deforestation: Lessons from Economic Models. The World Bank Research Observer, 14(1), 73–98. https://doi.org/10.1093/wbro/14.1.73."
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Re-rethinking the causes of deforestation: does the theory evolve?",
    "section": "",
    "text": "Another football field of forest will be lost in:\n\n\n   \n\n6\n\n\n\n\n\nForest loss since you opened this page: 0 acres\n\n\n\n\n\ninformation"
  }
]